---
title: Results
author: Chin Man KWAN
chapter: 4
in_context: True
knit: iheiddown::chapter_pdf
output: iheiddown::chapter_pdf
---

\setcounter{chapter}{3}

```{r python_setup}
library(reticulate)
```

```{python results_modules}
import warnings
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import jieba
import zhon.hanzi
import re
from itertools import chain

# Setting options for display and random seed
np.random.seed(1)
pd.set_option('display.max_columns', None)
sns.set_theme(context='paper')
warnings.filterwarnings(category=FutureWarning, action = 'ignore')
warnings.filterwarnings(category=UserWarning, action = 'ignore')
```

```{python import_dataset}
news_df = pd.read_csv("data/asylum_seekers_articles_final.csv")
num_of_articles = news_df.shape[0]
num_of_newspapers = news_df.Newspaper.nunique()
```

# Results

After the data collection process, there were in total `r py$num_of_articles` articles published in 2019 by `r py$num_of_newspapers` newspapers which reported on non-refoulement claimants residing in Hong Kong. In this section, I will first explore the data set, and then move onto sentiment analysis with machine learning models to find out whether the political camp of media outlets is associated with the polarity of the news articles towards asylum seekers.

## Exploratory data analysis (EDA)

### How does the number of news articles vary by political camps and month?

Starting with the number of articles by media outlets as shown in the left plot of figure 4.1, consistent with the study by @ngFramingIssueAsylum2019, Oriental Daily News continues to be the media outlet covering the most frequently on asylum seekers with 384 (or `r round(384/557 * 100, 2)`%) articles throughout 2019. By contrast, the second-most frequent publisher *Sing Tao Daily* only had 45 entries (or `r round(45/557 * 100, 2)`%) of the total number of articles published. Each of the other newspaper outlets only constituted to a small portion of news articles about non-refoulement claimants in 2019. Therefore, the issue of asylum seekers in Hong Kong still appeared to be the most salient for Oriental Daily News by 2019, evidenced by its unmatched volume of articles related to this topic vis-a-vis other media outlets.

On a higher level of political stance, the right plot of figure 4.1 indicates that largely due to the huge volume of articles by Oriental Daily News, the pro-Beijing camp dominated the coverage of asylum seekers in Hong Kong in 2019. Meanwhile, both neutral and pro-democracy newspaper outlets published similar amounts of articles throughout 2019, and both camps constituted to small proportions of the share of articles during the year. Even if we omitted the sheer volume of articles published by Oriental Daily News, the pro-Beijing media would still have `r 557-384` articles published altogether which was still considerably more than the quantity of articles by neutral and pro-democracy media outlets.

```{python, fig.cap="News articles on asylum seekers in 2019 by news outlet (left) and political camp (right)"}
fig, axes = plt.subplots(1,2)

# Plotting by-outlet amount of articles
articles_outlet = news_df.value_counts('Newspaper')
sns.barplot(x=articles_outlet, y=articles_outlet.index, ax=axes[0])
axes[0].set(xlabel='Number of articles (in base-10 log scale)', ylabel='', title='By newspaper', xscale="log")
for idx, value in enumerate(articles_outlet):
    axes[0].text(value + 5, idx + 0.2, value)

# Plotting by-camp amount of articles
sns.countplot(x='Political_camp', data=news_df, ax=axes[1])    
axes[1].set(xlabel='', ylabel='Number of articles', title='By political camp')
count_by_political_camp = news_df['Political_camp'].value_counts(sort=False)
for idx, value in enumerate(count_by_political_camp):
  axes[1].text(idx - 0.1, value + 5, value)
axes[1].set_xticklabels(['Pro-Beijing', 'Neutral', 'Pro-democracy'], rotation=60)
plt.tight_layout()
plt.show()
plt.clf()
```

It will also be intriguing to see how the number of articles might vary by month in 2019. As noted before, the anti-extradition law protest lasted mostly from June to November when numerous large-scale clashes between protesters and the police occurred. From figure 4.2, it appears that coincidentally, there were the fewest amounts of articles about asylum seekers published between August and November when some of the most intense clashes (notably the siege of the Hong Kong Polytechnic University in November 2019) took place.

```{python, fig.cap="Temporal patterns of the publication of news articles about asylum seekers in Hong Kong in 2019"}
articles_by_month = news_df.Month.value_counts(sort=False)
ax = sns.lineplot(x=articles_by_month.index, y=articles_by_month, color='tab:orange')
ax.set(xlabel='Month', ylabel='Number of articles', xticks=np.arange(1,13))
plt.tight_layout()
plt.show()
plt.clf()
```

In short, the majority of news articles about non-refoulement claimants in Hong Kong in 2019 were published by pro-Beijing media outlets, of which a huge proportion was from Oriental Daily News. Moreover, the number of articles by month was the lowest from August to November when the anti-extradition law witnessed some of the most large-scale and intense clashes.

### Polarities of the news articles

According to table 4.1, the polarity of the news articles about asylum seekers in Hong Kong in 2019 tilted towards negative, since only around 4.3% and 23.5% of articles respectively depicted asylum seekers positively and neutrally. The fact that the sentiment of the news articles in 2019 was skewed towards negativity implies that I will need to take class imbalance into account for modelling later. Political-camp-wise, pro-Beijing media outlets had over 70% of its articles depicting asylum seekers in Hong Kong in negative lights, whereas neutral and pro-democracy media outlets had their reportage evenly spread between neutral and positive articles (albeit they altogether constituted to only a small proportion of the total number of articles in 2019). While H~1~ shall be tested formally with machine learning models after including other control variables later, preliminary evidence suggests that the polarities of the news articles vary with the political camp that the outlets belong to.

```{python}
sentiment_camp = pd.crosstab(news_df.Political_camp, news_df.Sentiment, margins=True)
sentiment_camp.columns = ["Negative", "Neutral", "Positive", "All"]
```

```{r}
knitr::kable(py$sentiment_camp, digits = 4, caption="Polarities of the news articles on asylum seekers in Hong Kong in 2019")
```

### Presence of racial labels

Given the majority of asylum seekers in Hong Kong being non-ethnic Chinese, it will also be worth glimpsing whether the presence of racial labels for describing asylum seekers is associated with the sentiment of the news articles. Judging from figure 4.3 preliminarily, however, it appears that the patterns of the polarities are quite similar whether news articles contain racial labels or not, namely, most of the articles framed non-refoulement claimants negatively, some reported on events about this group of population neutrally, and only a small amount of articles were favourable towards asylum seekers residing in the city. In any case, the machine learning models can add the presence of racial labels as a control variable to test this potential association more formally later.

```{python, fig.cap="Presence of racial labels in the news articles by sentiment"}
ax = sns.countplot(x="Racial_label", hue="Sentiment", data=news_df)
ax.set(xlabel="Presence of racial labels", ylabel="Number of articles")
ax.set_xticklabels(["No", "Yes"])
plt.show()
plt.clf()
```

### Character lengths of news articles and titles

Lastly, let's look at the distribution of the character lengths of the titles and main texts of the news articles. According to figure 4.4 and table 4.2, it appears that both the title and main text lengths have right-skewed distributions. In other words, while most of the news articles on asylum seekers in Hong Kong in 2019 had relatively short titles and/or main texts, a few of them were considerably more verbose than the rest of the articles.

```{python, fig.cap="Distributions of the word counts of the articles' titles (left) and main texts (right)"}
fig, axes = plt.subplots(1, 2)

# Plotting distribution of title word count
sns.histplot(x='Title_length', data=news_df, ax=axes[0], color='tab:blue', alpha=0.5)
axes[0].set(xlabel='Word count', title='Article title')
mean_title_length = news_df.Title_length.mean()
axes[0].axvline(mean_title_length, alpha=0.5, linestyle = '-.', c='black', label='Mean of title length')
axes[0].legend()

# Plotting distribution of article word count
sns.histplot(x='Raw_article_length', data=news_df, ax=axes[1], color='tab:orange', alpha=0.5)
axes[1].set(xlabel='Word count (in thousands)', title='Raw article text')
mean_article_length = news_df.Raw_article_length.mean()
axes[1].axvline(mean_article_length, alpha=0.5, linestyle = '--', c='black', label='Mean of main text length')
axes[1].legend()

# Global setup
plt.tight_layout()
plt.show()
plt.clf()
```

```{python}
article_length_summary = news_df[['Title_length', 'Raw_article_length']].describe()
```

```{r}
knitr::kable(py$article_length_summary, col.names = c("Title", "Raw main text"), caption="Summary statistics of the word counts of the news articles' titles and main texts")
```

## Sentiment analysis

### Preprocessing

After making sense of the dataset with EDA, it is time to build the sentiment analysis model to see whether the political affiliation of news media outlets is associated with the polarities of the news articles after controlling for other variables. But first there are some preprocessing steps to be done so that the data are transformed into suitable formats as inputs for machine learning models. For starters, columns of the metadata should be excluded for being the inputs of the models. Note that I have also removed the `Newspaper` column since H~1~ is more interested in whether newspaper outlets of the pro-Beijing camp *as a whole* may hold more negative attitudes towards asylum seekers in Hong Kong vis-a-vis media outlets with other political stances. The removed metadata columns are: `Index, Date, Category, Page_number` and `Newspaper`.

```{python dropping_metadata_columns}
metadata_columns = ["Index", "Date", "Category", "Page_number", "Newspaper"]
news_df.drop(columns=metadata_columns, inplace=True)
```

Furthermore, I have binned `Month` into four even split yearly quarters (`Quarter`) to reduce the dimensionality of the dataset. A further note on the categorical features is that they will need to be transformed via one-hot encoding, meaning that each of them will be transformed into `n` variables, with `n` being the number of the original distinct values. Meanwhile, it would also be better to standardise the numerical features (i.e. other than `Political_camp` and `Quarter`) by centering theirs means at 0 for better model convergence, but the standardiser should only be fitted on the training set after splitting the data into the training and validation sets in order to avoid data leakage (the same is also true for creating the TF-IDF matrix). 20% of the observations in the dataset will be split into the test set for model validation later.

```{python creating_cat_variables}
# Making pro-Beijing become the reference category
news_df["Political_camp"] = pd.Categorical(news_df["Political_camp"], categories=['Pro-Beijing', 'Neutral', 'Pro-democracy'])  

# Binning the months into four quarters
def quarter(x):
  if x <= 3:
    return "Q1"
  elif x <= 6:
    return "Q2"
  elif x <= 9:
    return "Q3"
  else:
    return "Q4"
news_df["Quarter"] = pd.Categorical(news_df.Month.apply(quarter))
news_df.drop(columns="Month", inplace=True)

# One-hot encoding
news_df = pd.get_dummies(news_df, columns=["Political_camp", "Quarter"])
```

```{python train_and_test, echo=TRUE}
from sklearn.model_selection import train_test_split
X = news_df.drop(columns="Sentiment")
y = news_df.Sentiment
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=1)
```

The next step is to transform both the titles and main texts of the articles into a TF-IDF term-document matrix. Apart from joining the `Title` and `Text` columns together as the complete `Article`, I will also add additional words into the dictionary and remove stop words as well as punctuation for better tokenisation so that the `NMF` model can better discover the latent topics.

```{python joining_title_and_text, echo=TRUE}
# Train set
X_train["Article"] = X_train.Title.str.cat(news_df.Text, sep=" ")
X_train.drop(columns=["Text", "Title"], inplace=True)

# Test set
X_test["Article"] = X_test.Title.str.cat(news_df.Text, sep=" ")
X_test.drop(columns=["Text", "Title"], inplace=True)
```

```{python read_text_function, echo=TRUE}
def read_text(path):
    with open(path, 'r', encoding='utf-8') as file:
        text = file.readlines()
        text = [word.replace('\n', '') for word in text]
        return text
```

```{python adding_words_to_dict, echo=TRUE}
hk_politics_words = read_text('Coding/HKPolDict-master/merged.txt')  # Words related to Hong Kong politics
asylum_seeker_words = read_text('Coding/Asylum_seeker_words.txt')  # Words related to asylum seekers in Hong Kong
for word in chain(hk_politics_words, asylum_seeker_words):
  jieba.add_word(word)
```

```{python stop_word_list, echo=TRUE, include=FALSE}
stop_words_cantonese = read_text('Coding/text_cleaning/cantonese_stopwords.txt')
punctuations = [punc for punc in zhon.hanzi.punctuation]
stop_words_full = list(set(word for word in chain(stop_words_cantonese, punctuations)))  
```

To avoid data leakage as mentioned before, I will only fit the `TfidfVectorizer` and the `NMF` models on the train set (i.e. `X_train`) and then use the fitted instances to transform both the train and test sets. I set the number of latent topics (`n_components`) as 10 for the `NMF` model, and this is decided based on figure 4.5 which plots the reconstruction error measuring the difference of the values between the original TF-IDF matrix and the reconstructed version after NMF. Although there are certainly other valid choices of the number of latent topics to be discovered by NMF, `10` appears to be a reasonable choice as a compromise between finding out a wide variety of topics in the corpus and not fitting too much into the noise of the data.

```{python tokeniser_setup}
# tokenizer
def tokenize_zh(doc):
  return jieba.cut(doc)

# Preprocessor
def preprocessor_zh(doc):
  regex_punctuation = r"[\d+\s+\n\t]|[\s+\.\!\/_,$%^*(+\"\']+|[+——！，。？、~@#￥%……&*（）]+|[【】╮╯▽╰╭★→「」]+|[！，❤。～《》：（）【】「」？”“；：、]"
  return re.sub(regex_punctuation, "", doc)
```

```{python, fig.cap="Elbow plot of the reconstruction error of NMF as a function of the number of pre-specified latent topics"}
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.decomposition import NMF

# Creating the tfidf matrix from the training set
tfidf_vec = TfidfVectorizer(min_df = 0.02,  # Each token must appear in at least 2% of the documents
                            preprocessor=preprocessor_zh, 
                            tokenizer=tokenize_zh, 
                            stop_words=stop_words_full)
X_train_tfidf = tfidf_vec.fit_transform(X_train.Article)

# Plotting the reconstruction error according to the number of latent topics
reconstruct_error = []
for i in range(1, 21):
  nmf = NMF(n_components=i, max_iter=500, random_state=1)
  articles_nmf = nmf.fit(X_train_tfidf)
  reconstruct_error.append(nmf.reconstruction_err_)

ax = sns.lineplot(x=np.arange(1, 21), y=reconstruct_error)
ax.set(xlabel="Number of latent topics", ylabel="Reconstruction error", xticks=np.arange(1, 21))
plt.show()
plt.clf()

# Let's set n_components as 10 for nmf
nmf_10 = NMF(n_components=10, max_iter=500, random_state=1)
X_train_nmf = nmf_10.fit_transform(X_train_tfidf)
```

In order to make the latent topics generated by the NMF model be named more intuitive, I will inspect the 30 most prominent words of each latent topic (which are shown in figure 4.6) and then summarise each topic. Overall, the ten topics generated by NMF are more or less semantically coherent. Finally, I will transform the validation set's articles with the fitted instance of the TF-IDF and NMF models on the training set. The ten latent topics (from topics 1 to 10) that were discovered by the NMF model can be named as: *crimes, non-refoulement legal procedure, illegal labours, illegal gambling, drugs, illegal immigration, murder, robbery, South Asian settlements* and *problem and solution*[^1]. The presence of the themes in each article is represented by a value in each column of the respective theme, and the higher the value, the more emphasised a theme is in a given article. Note that multiple themes may co-exist within a news articles, albeit with different weights.

[^1]: In the context of whether and how non-refoulement claimants should be treated as a social problem and how this should be "solved" accordingly.

```{python nmf_topic_word_list, include=FALSE}
# Defining a function to extract the most prominent words in each topic
def topic_words(model, vectorizer, top_n_words):
  vocabulary = vectorizer.get_feature_names()
  for idx, topic in enumerate(model.components_):
    print(f"\nTopic {idx + 1}: ")
    print(" ".join([vocabulary[i] for i in topic.argsort()[:-top_n_words - 1: -1]]))

# Extracting the 30 most prominent words in each topic
_ = topic_words(nmf_10, tfidf_vec, 30)  # the output is shown in figure 4.6
```

```{r, fig.cap="Word list of each of the topics generated by the NMF model, note: the importance of each word in each topic is arranged in descending order from left to right", out.width="100%"}
knitr::include_graphics("figures/nmf_topic_words_list.png")
```

```{python latent_topic_df, include=FALSE}
# Naming the latent topics more precisely
topics_list = ["Crimes", "Non-refoulement legal procedure", "Illegal labours", "Illegal gambling", "Drugs", "Illegal immigration", "Murder", "Robbery", "South Asian settlements", "Problem and solution"]

# Concatenating the NMF DataFrame for the training set
X_train_nmf_df = pd.DataFrame(X_train_nmf, index=X_train.index, columns=topics_list)
X_train_final = pd.concat([X_train, X_train_nmf_df], axis=1)
X_train_final.drop(columns="Article", inplace=True)

# Concatenating the NMF DataFrame for the validation set
X_test_tfidf = tfidf_vec.transform(X_test.Article)
X_test_nmf = nmf_10.transform(X_test_tfidf)
X_test_nmf_df = pd.DataFrame(X_test_nmf, index=X_test.index, columns=topics_list)
X_test_final = pd.concat([X_test, X_test_nmf_df], axis=1)
X_test_final.drop(columns="Article", inplace=True)

```

### Training the model

After the above preprocessing steps, it is time to train a model that adequately predicts the relations between the features and the sentiment of the articles before finding out the importance of the political camp as the main independent variable. To facilitate the decision of which model to use and model tuning, I will first run some baseline models with the default hyper-parameters, except that I have adjusted the weights of each class in the dependent variable due to class imbalance and also set the `early_stopping_rounds` argument to `5` to prevent overfitting while training the `XGBoost` model [^2]. Moreover, tree-based models (i.e. random forest and xgboost) do not necessarily need to have the numerical features standardised, and thus only the categorical columns need to be one-hot encoded. The baseline models will be compared based on their performance on the *macro* average f1 score (which is simply the unweighted average of per-class f1 scores, @pedregosaScikitlearnMachineLearning2011) on the testing set, and the 5-fold cross validation f1 scores are also provided for reference. I chose the macro average f1 score because there is no apparent reason for treating the prediction of one polarity of the news articles to be more important than others.

[^2]: For the complete documentation of the default parameters of the models used in this thesis, refer to the websites of [scikit-learn](scikit-learn:%20machine%20learning%20in%20Python%20—%20scikit-learn%201.0.2%20documentation) and [XGBoost Documentation --- xgboost 1.5.1 documentation](https://xgboost.readthedocs.io/en/stable/).

```{python evaluation_metric}
from sklearn.model_selection import cross_val_score, StratifiedKFold
from sklearn.metrics import f1_score, make_scorer

# Defining the kfold strategy
five_fold_cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=1)

# Utility function for evaluating the model's performance in cross validation and test set in terms of log loss
def evaluate_model_f1(model, model_name: str, cv=five_fold_cv, X_train=X_train_final, X_test=X_test_final, y_train=y_train, y_test=y_test):
  y_pred = model.predict(X_test)
  cv_f1_score = np.mean(cross_val_score(model, X_train, y_train, cv=cv, scoring=make_scorer(f1_score, average="macro")))
  test_f1_score = f1_score(y_test, y_pred, average="macro")
  return {"5-fold cv f1 score": cv_f1_score, "Test set f1 score": test_f1_score}
  
```

```{python preprocessing}
from sklearn.preprocessing import StandardScaler
from sklearn.compose import ColumnTransformer

# Separating the columns for respective preprocessing steps
numeric_columns = [col for col in X_train_final.columns if X_train_final[col].dtype in ["int64", "float64"] and col != "Racial_label"]

# Preprocessor for linear models
stand_preprocessor = ColumnTransformer([("standardiser", StandardScaler(), numeric_columns)],
                                         remainder='passthrough')

```

```{python baseline_models}
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.ensemble import RandomForestClassifier
from sklearn.pipeline import Pipeline
from sklearn.utils.class_weight import compute_sample_weight
import xgboost as xgb

# Logistic regression pipeline
log_reg_baseline = Pipeline([("preprocess", stand_preprocessor), 
                             ("log_reg", LogisticRegression(random_state=1,  
                                                            class_weight="balanced"))])
_ = log_reg_baseline.fit(X_train_final, y_train)
log_reg_base_result = evaluate_model_f1(log_reg_baseline, "baseline logistic regression")

# SVM pipeline
svm_baseline = Pipeline([("preprocess", stand_preprocessor), 
                         ("svm", SVC(probability=True, class_weight="balanced", random_state=1))])
_ = svm_baseline.fit(X_train_final, y_train)
svm_base_result = evaluate_model_f1(svm_baseline, "baseline support vector machine")

# Random forest pipeline
rf_baseline = RandomForestClassifier(class_weight="balanced", random_state=1, criterion="entropy")
_ = rf_baseline.fit(X_train_final, y_train)
rf_base_result = evaluate_model_f1(rf_baseline, "baseline random forest")

# xgboost pipeline
xgb_sample_weight = compute_sample_weight(class_weight="balanced", y=y_train)
xgboost_baseline = xgb.XGBClassifier(objective="multi:softmax",
                                     eval_metric="mlogloss",
                                     seed=1, 
                                     use_label_encoder=False)
_ = xgboost_baseline.fit(X_train_final, 
                         y_train, 
                         sample_weight=xgb_sample_weight, 
                         eval_set=[(X_test_final, y_test)], 
                         early_stopping_rounds=5, 
                         verbose=0)
xgb_base_result = evaluate_model_f1(xgboost_baseline, "baseline XGBoost")

# Creating the DataFrame of the baseline results
baseline_f1_score_df = pd.DataFrame([log_reg_base_result, svm_base_result, rf_base_result, xgb_base_result], index=["Logistic regression", "SVM", "Random forest", "XGBoost classifier"])

```

```{r}
knitr::kable(py$baseline_f1_score_df, caption = "Log loss on 5-fold cv and test set for the 4 baseline models")
```

Table 4.3 contains the performance of both the 5-fold cross validation and test set macro average f1 score scores of the four baseline models. It seems that XGBoost and logistic regression perform better out of all the baseline models. Eventually, I decided to proceed with tuning the XGBoost model because its tree-based nature allows it to capture potential non-linear relations between the features and the target variable compared to logistic regression which is a linear classifier.

Judging from the difference of the f1 scores between cross validation and test set by the XGBoost model, however, it seems that the baseline model may have under-fitted the data because the cross validation f1 score is considerably lower than the test set f1 score. I will therefore perform hyper-parameter tuning of the XGBoost model to see if it can be fitted better for the training data[^3]. The comparison of the baseline and tuned model's performance in f1 score can be found in table 4.4. It seems that the tuned model now fits the training data much better because the f1 score on cross validation is very close to that on the test data. Table 4.5 further breaks down the f1 score of the tuned xgboost model on the testing data. Although the model performs relatively worse in predicting news articles with neutral sentiments since the f1 score for this class is only about 0.69, it performs quite well on predicting the classes of positive and negative since these two classes' per-class f1 score are both close to 0.9. I will thus calculate the SHAP values of the features with the tuned model given its better performance overall on fitting the training data and predicting unseen test data.

[^3]: The code for tuning the model can be found in the appendix. Hyper-parameters of the tuned model used here are: {'objective': 'multi:softprob', 'use_label_encoder': False, 'base_score': 0.5, 'booster': 'gbtree', 'colsample_bylevel': 1, 'colsample_bynode': 1, 'colsample_bytree': 0.6000000000000001, 'enable_categorical': False, 'gamma': 0.5, 'gpu_id': -1, 'importance_type': None, 'interaction_constraints': '', 'learning_rate': 0.37, 'max_delta_step': 0, 'max_depth': 5, 'min_child_weight': 3.0, 'missing': nan, 'monotone_constraints': '()', 'n_estimators': 40, 'n_jobs': 16, 'num_parallel_tree': 1, 'predictor': 'auto', 'random_state': 1, 'reg_alpha': 0, 'reg_lambda': 12.0, 'scale_pos_weight': None, 'subsample': 0.7000000000000001, 'tree_method': 'exact', 'validate_parameters': 1, 'verbosity': None, 'eval_metric': 'mlogloss'}. To reproduce this model, simply create an `XGBClassifier` instance, copy the above hyperparameters as a dictionary and then use the .set_params(\*\*dict) method on the `XGBClassifier` instance.

```{python comparing_tuned_and_baseline}
import pickle
from sklearn.metrics import classification_report

# Loading the tuned model
xgboost_tuned = pickle.load(open("models/xgb_clf_tuned_II.pkl", "rb"))

# Getting the performance of the tuned model
xgb_tuned_result = evaluate_model_f1(xgboost_tuned, "Tuned xgboost")

# Creating DataFrame to compare baseline and tuned macro f1 score
xgb_compare_df = pd.DataFrame([xgb_base_result, xgb_tuned_result], index=["Baseline", "Tuned"])

# Creating classification_report for the tuned model
xgb_tuned_clf_report = pd.DataFrame(classification_report(y_test, xgboost_tuned.predict(X_test_final), output_dict=True)).T
```

```{r}
knitr::kable(py$xgb_compare_df, digits=4, caption="Comparison of the f1 scores on 5-fold cross validation and test set between the baseline and tuned XGBoost models")

```

```{r}
knitr::kable(py$xgb_tuned_clf_report, digits=4, caption="Classification report on the f1 score of the tuned XGBoost model on the testing data")

```

## Is the pro-Beijing camp more likely to portray asylum seekers in 2019 more negatively than other outlets?

```{python defining_shap_plot_functions, include=FALSE}
# Setting up the shap values 
import shap
xgb_explainer = shap.TreeExplainer(xgboost_tuned)
xgb_shap_values = xgb_explainer.shap_values(X_train_final)

# Getting the expected probability of each class in one-versus-all manner
expected_0, expected_1, expected_2 = xgb_explainer.expected_value

# Defining the summary plot function
def shap_summary_plot(class_label=None, **kwargs):
  if class_label == None:
    shap.summary_plot(xgb_shap_values, X_train_final, show=False, **kwargs)
  else:
    shap.summary_plot(xgb_shap_values[class_label], X_train_final, show=False, cmap="Greys", **kwargs)
  plt.tight_layout()
  plt.show()
  plt.clf()
  
# Defining the dependence plot function
def shap_dependence_plot(feature_list, class_label, interaction_feature=None, **kwargs):
  fig, axes = plt.subplots(2, 2)
  for feature, ax in zip(feature_list, axes.ravel()):
    shap.dependence_plot(feature, xgb_shap_values[class_label], X_train_final, ax=ax, title=feature, show=False, cmap="Greys", interaction_index=interaction_feature, **kwargs)
    ax.set(xlabel="", ylabel="")
  fig.supxlabel("Feature values")
  fig.supylabel("SHAP values")
  plt.tight_layout()
  plt.show()
  plt.clf()
```

This section will move onto report the findings of how important each feature contributed to the model's prediction of the sentiments of the news articles on asylum seekers in Hong Kong by media outlets in 2019. Apart from reporting on whether affiliation with the pro-Beijing camp of media outlets is an influential feature for predicting each of the polarities, I will also mention other intriguing findings afterwards. The data points used for constructing the SHAP values are from the training set (`X_train_final`). According to the documentation of the `TreeExplainer` class used for generating SHAP values of tree-based ensemble models [@ShapTreeExplainerSHAP], in the case of classification tasks, the model output explained by SHAP values is the *log odds ratio.* Consequently, positive SHAP values (i.e. increasing log odds ratio) mean that an observation is more likely to belong to a certain class, whereas negative SHAP values implies that an observation is less likely to belong to a certain class.

### Overall magnitudes of SHAP values for each feature

With the trained model at hand, we can now answer whether H~1~ is supported by the model's results using SHAP values. According to figure 4.7, we can see that on the level of the whole model, whether a media belongs to the pro-Beijing camp or not (`Political_camp_Pro-Beijing`) is the third most important features in predicting the sentiment of a news article, and its magnitude of SHAP values in affecting the model's output is only lower than those of two news article themes about asylum seekers (i.e., problem and solution and murder) while being slightly higher that that of articles on crimes. Furthermore, within the bar of the SHAP values of the `Political_camp_Pro-Beijing` feature, we can see that pro-Beijing affiliation of media outlets is considerably more informative for predicting whether an article has positive polarity (`Class 2`) or not and has negative polarity (`Class 0`) or not, but no so much while predicting if an article has a neutral polarity (`Class 1`) or not. Therefore, it seems that pro-Beijing media outlets do tend to differ their sentiment on the reportage of asylum seekers in Hong Kong compared to other non-pro-Beijing outlets.

It should also be noted that contrary to expectation, `Racial_label` is not an informative feature in predicting the sentiment of the articles since its magnitude of SHAP values is at the bottom five out of all features. One potential reason may be that the connotation between non-ethnic Chinese and non-refoulement claimants is quite salient that these two labels are often used together by newspaper outlets no matter the sentiment or attitudes towards asylum seekers in Hong Kong. Furthermore, the sentiments of the news articles do not seem to correlate with in which quarter they were published, and so is the length of the titles of news articles.

```{python, out.width="100%", out.height="50%", fig.cap="Magnitudes of SHAP values of each feature for all classes"}
shap_summary_plot()
```

Nevertheless, figure 4.7 does not really show the *direction* of the SHAP values of each feature in affecting the model's output. We can therefore use the beeswarm summary plots from the `shap` package which also shows the directions of SHAP values for predicting each class in the dependent variable as each feature's value changes. The importance of the features is arranged in descending order on the y-axis from top (the most important) to bottom (the least important). In a SHAP value beeswarm summary plot, dots in black mean the value of a feature is high (or present in case of a binary feature, e.g. one-hot-encoded columns), whereas those in white mean the value of a feature is low (or absent in the case of a binary feature). Moreover, dependence plots which zooms in the relations between the feature values (on the x-axis) and SHAP values (on the y-axis) of the eight most important features will be provided.

A little note on how to interpret the findings from the beeswarm summary and dependence plots. In essence, each plot will show the SHAP values of each feature to assess whether a feature's value away from its baseline will increase or decrease the model's prediction of the log odds ratio that an observation belongs to a certain class. For instance, if feature A's SHAP value for class 0 increases as its value becomes larger, then this means the model will predict that an observation is more likely to be class 0 as feature A's value becomes larger.

### SHAP values of predicting negative news articles

```{python, out.width = "100%", out.height="50%", fig.cap="The SHAP values of the features in the prediction of whether an article has a negative polarity"}
shap_summary_plot(0)
```

According to figure 4.8, pro-Beijing affiliation (`Political_camp_Pro-Beijing`) is the third most important feature in predicting whether a news article reports on asylum seekers in Hong Kong negatively or not. Consistent with the expectation in H~1~, articles published by pro-Beijing media newspapers are more likely to report on asylum seekers negatively than those by outlets with different political orientations. Conversely, albeit with less magnitude in SHAP values, media outlets with neutral (`Political-camp-Neutral`) or pro-Democracy (`Political_camp_Pro-democracy`) stances are less likely to publish negative articles on non-refoulement claimants. When it comes to negative articles, therefore, political affiliations of media outlets in Hong Kong do matter, meaning that pro-Beijing camp newspapers are more likely to depict non-refoulement claimants negatively than their counterparts in other political stances[^4].

[^4]: Of course, Oriental Daily News contributed to a huge volume of reportage on non-refoulement claimanats within the pro-Beijing camp. I will explain why it is not appropriate to re-run the model without including entries from this outlet in the Discussion section below.

```{python, out.width="100%", fig.cap="Dependence plot of the eight most important features for predicting negative polarity of news articles"}
neg_features_I = ["Crimes", "Murder", "Political_camp_Pro-Beijing", "Non-refoulement legal procedure"]
shap_dependence_plot(neg_features_I, 0)

neg_features_II = ["Problem and solution", "Illegal labours", "Raw_article_length", "Drugs"]
shap_dependence_plot(neg_features_II, 0)
```

Figures 4.9 and 4.10 further zoom into the SHAP values of the eight most important features for predicting the negative polarity of the news articles. Some interesting patterns are observed here. Firstly, articles which are more related to crimes (`Crimes`), the policies of non-refoulement claims (`Non-refoulement policy`) and problem and solution (`problem and solution`) are more likely to report on asylum seekers negatively. These directions are consistent with the most prominent words found in these topics. Specifically, both `Non-refoulement policy` and `problem and solution` have the derogatory term "fake refugee" as one of the topic words, whereas the theme `Crimes` found the noun phrase "South Asian army" (*nan2ya4 bing1tuan2*) as two of the topic words which together attempt to conflate non-refoulement claimants with the image of them being South Asians coming to Hong Kong en masse for committing crimes.

Interestingly, more verbose news articles (i.e. having longer `Raw_article_length`) are also predicted to be more likely for holding negative opinion towards non-refoulement claimants. Lastly, a more pronounced presence of the themes of `Illegal labours` and `Drugs` in a news article appear to be more likely to have a negative polarity against non-refoulement claimants as shown by their SHAP values. Meanwhile, articles focusing on the theme of \`Murder\` are predicted to be *unlikely* for carry a negative polarity.

### SHAP values of predicting neutral news articles

```{python, out.width = "100%", out.height="50%", fig.cap="The SHAP values of the features in the prediction of whether an article has a negative polarity"}
shap_summary_plot(1)
```

Figure 4.11 shows the SHAP values of each feature in contributing to the prediction of neutral news articles towards non-refoulement claimants. Contrary to the case of the prediction of negative news articles, the political affiliation of newspaper outlets do not contribute substantively to the model's output, as all three political orientations of newspaper outlets covered in this thesis are located at the bottom five positions on the y-axis, and their SHAP values are essentially zero.

```{python, out.width="100%", fig.cap="Dependence plot of the eight most important features for predicting neutral polarity of news articles"}
neu_features_I = ["Problem and solution", "Robbery", "Crimes", "South Asian settlements"]
shap_dependence_plot(neu_features_I, 1)

neu_features_II = ["Murder", "Raw_article_length", "Illegal immigration", "Illegal labours"]
shap_dependence_plot(neu_features_II, 1)

```

Rather, some topics of the news articles are more informative in predicting whether news articles are only reporting incidents without much interpretation by the journalists. We can look into this further in figures 4.12 and 4.13. For instance, articles more related to `problem and solution`, `Crimes` or `South Asian settlements` are less likely to be written neutrally by newspaper outlets covered in the dataset. In other words, these topics are more likely to be loaded with particular sentiments when they were reported by newspapers in 2019. On the other hand, articles related to `Robbery`, `Illegal immigration` and `Murder` were generally less loaded with sentiments by newspapers. Moreover, as the length of the news articles gets longer, it becomes less likely that the articles are not loaded with conspicuous sentiments towards asylum seekers in Hong Kong.

### SHAP values of predicting positive news articles

```{python, out.width="100%", out.height="50%", fig.cap="The SHAP values of the features in the prediction of whether an article has a positive polarity"}
shap_summary_plot(2)
```

Lastly, figure 4.14 shows the SHAP values of the feature in predicting whether news articles have a positive polarity towards non-refoulement claimants or not. This time, pro-Beijing affiliation becomes the second most crucial feature for predicting whether a news articles depicts non-refoulement claimants positively. As expected in H~1~ again, a newspaper outlet is less likely to publish positive articles about asylum seekers if it is associated with the pro-Beijing camp. Moreover, albeit with a smaller magnitude of SHAP values, pro-Democracy newspaper outlets are predicted to be more likely to have favourable reportage on non-refoulement claimants. Meanwhile, neutral newspaper outlets are neither more nor less likely to publish positive articles while reporting on asylum seekers in Hong Kong. In sum, pro-Beijing affiliation of newspaper media in Hong Kong is quite informative for predicting whether an article will portray non-refoulement claimants in positive light, and it is inferred from the model that pro-Beijing outlets are generally less likely to give positive coverage of this group of population.

```{python, out.width="100%", fig.cap="Dependence plot of the eight most important features for predicting positive polarity of news articles"}
pos_features_I = ["Problem and solution", "Political_camp_Pro-Beijing", "Murder", "Illegal labours"]
shap_dependence_plot(pos_features_I, 2)

pos_features_II = ["Drugs", "Illegal gambling", "Raw_article_length", "Crimes"]
shap_dependence_plot(pos_features_II, 2)
```

Figures 4.15 and 4.16 again show the eight most important features on influencing the model to predict if a news article has positive polarity or not. Save for `problem and solution` (which will be explored later), quite a number of features (namely, `Murder`, `Illegal labours`, `Drugs`, `Illegal gambling` and `Crimes`) do not exhibit clear relationships between the feature and SHAP values, since the dependence plots of these features show rather "flat" trends of the correlation of these two values. Likewise, after omitting the few outliers in the upper-right quadrant of the dependence plot, `Raw_article_length` also do not show very clear relations of how increasing the length of news articles may affect the SHAP values.

The pattern of the correlation between feature and SHAP values for the theme of `problem and solution`, however, merits some deeper investigation because its SHAP value

```{python}
shap.dependence_plot("Political_camp_Pro-Beijing", xgb_shap_values[2], X_train_final, interaction_index="Problem and solution", cmap="Greys", show=False)
plt.tight_layout()
plt.show()
plt.clf()
```

```{python}
pos_train_idx = y_train[y_train == 2].index
pos_train_article = X_train.query("@X_train.index in @pos_train_idx")
```

Perhaps one reason for the inability of the model to discover the relationships between feature values and their impact on the model's output is due to the extremely small number of positive articles present in the whole dataset (only 24 out of 557 entries). Accordingly, the lack of sufficient cases of

Therefore, even if the `problem and solution` and `Political_camp_Pro-Beijing` features exhibited relatively clearly relationships between the feature and SHAP values in the dependence plots, these findings should be taken with a grain of salt since the variance of

## Discussion

Based on the results of SHAP values from the model, it can be concluded that H~1~ is supported by the empirical evidence. In other words, a few years since the issue of non-refoulement claimants has become more visible in the public debate, pro-Beijing newspaper outlets were still more likely to portray asylum seekers negatively than outlets with other political affiliations by 2019 after accounting for how the themes of the news articles might also affect the polarity of the reportage. Moreover, certain themes of news articles about non-refoulement claimants were associated with

There are also a few points worth mentioning based on the model's results. For starters, although the class imbalance problem of the `Sentiment` of the news articles on non-refoulement claimants in 2019 posed a few obstacles during the modelling process, this also suggests that printed newspapers in Hong Kong did not provide as much positive or even neutral coverage as it is the case for negative coverage. This phenomenon is likely contributed by the fact that *Oriental Daily News* was the sole major newspaper for publishing on the issue of asylum seekers in Hong Kong (as *the Sun* is defunct since 2016), and it extensively used the term "fake refugee" among other derogatory descriptions to delegitimise the non-refoulement claimants, similar to what @ngFramingIssueAsylum2019 observe on headline articles about asylum seekers when the issue first gained its salience in 2016. Thus, even when Hong Kong witnessed

Another point worth discussing will be how the Pro-Beijing camp's emphasis on the ethnic homogeneity of Hong Kong citizens and mainland Chinese while defining the city's national identity may contribute to the negative tendency of reportage on non-refoulement claimants by the pro-Beijing newspaper media.

Admittedly, there also exist some articles by pro-Beijing newspapers which aim to differentiate the so-called "fake refugees" from ethnic minorities who are "legitimate" citizens in Hong Kong. For example, the below article by *Ta kung Pao*

Nevertheless, given the , it is unlikely that

One question about the validity of the analysis result can be that the vast volume of publication about asylum seekers by Oriental Daily News might have skewed the pro-Beijing media's attitudes towards asylum seekers towards the negative ends. While it is certainly a fact that Oriental Daily News covered non-refoulement claimants disproportionately throughout 2019, it would not be ideal to re-run the model after excluding the news entries by this outlet for two reasons. Firstly, Oriental Daily News is after all part of the pro-Beijing media network (and in the context of reportage on asylum seekers within the city, a very important one) whose owners are co-opted by the Beijing Government according to @leeChangingPoliticalEconomy2018, and thus dropping out this particular newspaper will lose a lot of information about how the pro-Beijing overall may portray asylum seekers. Secondly, the fact that Oriental Daily News had the second largest market share among other paid newspapers and was the third most popular media by 2019 [@adintimeMostWidelyreadMagazine2021] implies that dropping out this media outlet

However,
