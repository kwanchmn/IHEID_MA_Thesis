---
####################
## Metadata       ##
####################
doctitle: |
  When central-peripheral relations intersect with the asylum-seeking regime: how media outlets from different political camps report on asylum seekers in Hong Kong
firstnames: Chin Man
lastnames: Kwan
thesisno: 12345
phd: false # Or: false for Masters
department: The International Relations/Political Science Department
degreedate: January 2022
supervisor: Prof. Sungmin Rho
secondreader: Prof. Ravinder Bhavnani
# examiner: Aragorn II Elessar # only required if PhD; otherwise delete or comment out
# This is the YAML (YAML Ain't Markup Language) header that includes 
# metadata used to produce the document. 
# Be careful with spacing in this header!
# Two spaces at the start of the line create indentation,
# which embeds the parameter in a hierarchy.

####################
## Front Matter   ##
####################
dedication: For my loved ones #If you'd prefer to not include a Dedication, for example, simply delete the section entirely, or silence them (add # before each line). 
resume: | # otherwise known as the abstract
  `r paste(readLines("front-matter/_abstract.Rmd"), collapse = '\n  ')`
acknowledgements: |
  `r paste(readLines("front-matter/_acknowledgements.Rmd"), collapse = '\n  ')`
abbreviations: "front-matter/abbreviations" # path to .tex file with abbreviations
toc-depth: 6 # depth of heading to include in table of contents
lof: true # list of figures in front matter?
lot: true # list of tables in front matter?
mini-toc: false  # mini-table of contents at start of each chapter? (this just prepares it; you must also add \minitoc after the chapter titles)
mini-lot: false  # mini-list of tables by start of each chapter?
mini-lof: false  # mini-list of figures by start of each chapter?

####################
## Bibliography   ##
####################
bibliography: bib/references.bib #Set your bibliography file here.
bibliography-heading-in-pdf: References
bib-style: authoryear # See https://www.overleaf.com/learn/latex/biblatex_citation_styles for a list of the commonly used built-in citations styles of biblatex.
bib-sorting: nyt #See https://www.overleaf.com/learn/latex/Articles/Getting_started_with_BibLaTeX for different bibliography sorting options.
citeall: true #Set this to true if you want all elements in your .bib file to appear in your bibliography (i.e. all R-packages you used).

#####################
## PDF Formatting  ##
#####################
draft: true # add DRAFT mark in the footer and line numbering?
page-layout: oneside #'oneside' for PDF output (equal margins), 
# or 'twoside' for two-sided binding (mirror margins and blank pages) 
hidelinks: false #if false, the PDF output highlights clickable links with a colored border 
# you will probably want to set this to true for PDF version you wish to physically print
knit: iheiddown::thesis_pdf
#####################
## Output Options  ##
#####################
output: iheiddown::thesis_pdf
link-citations: true
documentclass: book
---

# examiner: Aragorn II Elessar # only required if PhD; otherwise delete or comment out

Placeholder


## Metadata       ##
## Front Matter   ##
## Bibliography   ##
## PDF Formatting  ##
## Output Options  ##

<!--chapter:end:index.Rmd-->


# Introduction {#intro}

Placeholder



<!--chapter:end:01-introduction.Rmd-->


# Literature Review {#theory}

Placeholder


## Theories
### What is national identity?
### Impact of conceptions of national identity on attitudes towards immigrants
### Chineseness in Hong Kong
## Empirics
### How do different political camps in Hong Kong perceive national identity?
#### The pro-Beijing camp
#### The pro-Democracy camp (or pan-democrats)
#### The localist camp
### Asylum seekers in Hong Kong
#### Hong Kong's policy regime
#### Responses by political camps

<!--chapter:end:02-theory.Rmd-->


# Methods {#methods}

Placeholder


## Hypotheses
## Data collection
## Modelling
## Operationalisation
### Main independent variable
### Dependent variable
### Control variables

<!--chapter:end:03-method.Rmd-->

---
title: Results
author: Chin Man KWAN
chapter: 4
in_context: True
knit: iheiddown::chapter_pdf
output: iheiddown::chapter_pdf
---

\setcounter{chapter}{3}

```{r python_setup}
library(reticulate)
```

```{python results_modules}
import warnings
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import jieba
import zhon.hanzi
import re
from itertools import chain

pd.set_option('display.max_columns', None)
sns.set_theme(context='paper')
warnings.filterwarnings(category=FutureWarning, action = 'ignore')
```

```{python import_dataset}
news_df = pd.read_csv("Coding/asylum_seekers_articles_final.csv")
num_of_articles = news_df.shape[0]
num_of_newspapers = news_df.Newspaper.nunique()
```

# Results

After searching for articles which contain at least one of the keywords and are relevant to asylum seekers residing in Hong Kong, there were in total `r py$num_of_articles` articles published in 2019 by `r py$num_of_newspapers` newspapers. In this section, I will first explore the data set preliminarily, and then move onto sentiment analysis with machine learning models to find out whether the political camp of media outlets is associated with the polarity of the news articles towards asylum seekers.

## Exploratory data analysis (EDA)

### How does the number of news articles vary by political camps and month?

Starting with the number of articles by media outlets as shown in the left plot of figure 4.1, consistent with the study by @ngFramingIssueAsylum2019, Oriental Daily News continues to be the media outlet covering the most frequently on asylum seekers with 384 (or `r round(384/557 * 100, 2)`%) articles throughout 2019. By contrast, the second-most frequent publisher *Sing Tao Daily* only had 45 entries (or `r round(45/557 * 100, 2)`%) of the total number of articles published. Each of the other newspaper outlets only constituted to a small portion of news articles about non-refoulement claimants in 2019. Therefore, the issue of asylum seekers in Hong Kong still appeared to be the most salient for Oriental Daily News by 2019, evidenced by its unmatched volume of articles related to this topic vis-a-vis other media outlets.

On a higher level of political stance, the right plot of figure 4.1 indicates that largely due to the huge volume of articles by Oriental Daily News, the pro-Beijing camp dominated the coverage of asylum seekers in Hong Kong in 2019. Meanwhile, both neutral and pro-democracy newspaper outlets published similar amounts of articles throughout 2019, and both camps constituted to small proportions of the share of articles during the year. Even if we omitted the sheer volume of articles published by Oriental Daily News, the pro-Beijing media would still have `r 557-384` articles published altogether which was still considerably more than the quantity of articles by neutral and pro-democracy media outlets.

```{python, fig.cap="News articles on asylum seekers in 2019 by news outlet (left) and political camp (right)"}
fig, axes = plt.subplots(1,2)

# Plotting by-outlet amount of articles
articles_outlet = news_df.value_counts('Newspaper')
sns.barplot(x=articles_outlet, y=articles_outlet.index, ax=axes[0])
axes[0].set(xlabel='Number of articles (in base-10 log scale)', ylabel='', title='By newspaper', xscale="log")
for idx, value in enumerate(articles_outlet):
    axes[0].text(value + 5, idx + 0.2, value)

# Plotting by-camp amount of articles
sns.countplot(x='Political_camp', data=news_df, ax=axes[1])    
axes[1].set(xlabel='', ylabel='Number of articles', title='By political camp')
count_by_political_camp = news_df['Political_camp'].value_counts(sort=False)
for idx, value in enumerate(count_by_political_camp):
  axes[1].text(idx - 0.1, value + 5, value)
axes[1].set_xticklabels(['Pro-Beijing', 'Neutral', 'Pro-democracy'], rotation=60)
plt.tight_layout()
plt.show()
plt.clf()
```

Lastly, it will also be intriguing to see how the number of articles might vary by month in 2019. As noted before, the anti-extradition law protest lasted mostly from June to November when numerous large-scale clashes between protesters and the police occurred. From figure 4.2, it appears that coincidentally, there were the fewest amounts of articles about asylum seekers published between August and November when some of the most intense clashes (notably the *siege* of the Hong Kong Polytechnic University in November 2019) took place. Although investigating whether the number of news articles about asylum seekers may be correlated with that about the anti-extradition law protest would be out of the scope of this paper, this could be a research question to be pursued in another occasion.

```{python, fig.cap="Temporal patterns of the publication of news articles about asylum seekers in Hong Kong in 2019"}
articles_by_month = news_df.Month.value_counts(sort=False)
ax = sns.lineplot(x=articles_by_month.index, y=articles_by_month, color='tab:orange')
ax.set(xlabel='Month', ylabel='Number of articles', xticks=np.arange(1,13))
plt.tight_layout()
plt.show()
plt.clf()
```

In short, the majority of news articles about non-refoulement claimants in Hong Kong in 2019 were published by pro-Beijing media outlets, of which a huge proportion was from Oriental Daily News. Moreover, the number of articles by month was the lowest from August to November when the anti-extradition law witnessed some of the most large-scale and intense clashes.

### Polarities of the news articles

According to table 4.1, the polarity of the news articles about asylum seekers in Hong Kong in 2019 tilted towards negative, since only around 4.3% and 23.5% of articles respectively depicted asylum seekers positively and neutrally. The fact that the sentiment of the news articles in 2019 was skewed towards negativity implies that I will need to take class imbalance into account for modelling later. Political-camp-wise, pro-Beijing media outlets had over 70% of its articles depicting asylum seekers in Hong Kong in negative lights, whereas neutral and pro-democracy media outlets had their reportage evenly spread between neutral and positive articles (albeit they altogether constituted to only a small proportion of the total number of articles in 2019). While H~1~ shall be tested formally with machine learning models after including other control variables later, preliminary evidence suggests that the polarities of the news articles vary with the political camp that the outlets belong to.

```{python}
sentiment_camp = pd.crosstab(news_df.Political_camp, news_df.Sentiment, margins=True)
sentiment_camp.columns = ["Negative", "Neutral", "Positive", "All"]
```

```{r}
knitr::kable(py$sentiment_camp, digits = 4, caption="Polarities of the news articles on asylum seekers in Hong Kong in 2019")
```

### Presence of racial labels

Given the majority of asylum seekers in Hong Kong being non-ethnic Chinese, it will also be worth glimpsing whether the presence of racial labels for describing asylum seekers is associated with the sentiment of the news articles. Judging from figure 4.3 preliminarily, however, it appears that the patterns of the polarities are quite similar whether news articles contain racial labels or not, namely, most of the articles framed non-refoulement claimants negatively, and then some reported on events about this group of population neutrally, and finally only a small amount of articles were favourable towards asylum seekers residing in the city. In any case, the machine learning model can add the presence of racial labels as a control variable to test this potential association more formally later.

```{python}
ax = sns.countplot(x="Racial_label", hue="Sentiment", data=news_df)
ax.set(xlabel="Presence of racial labels", ylabel="Number of articles")
ax.set_xticklabels(["No", "Yes"])
plt.show()
plt.clf()
```

### Character lengths of news articles and titles

Lastly, let's look at the distribution of the character lengths of the titles and main texts of the news articles. According to figure 4.4 and table 4.2, it appears that both the title and main text lengths have right-skewed distributions. In other words, while most of the news articles on asylum seekers in Hong Kong in 2019 had relatively short titles and/or main texts, a few of them were considerably more verbose than the rest of the articles.

```{python, fig.cap="Distributions of the word counts of the articles' titles (left) and main texts (right)"}
fig, axes = plt.subplots(1, 2)

# Plotting distribution of title word count
sns.histplot(x='Title_length', data=news_df, ax=axes[0], color='tab:blue', alpha=0.5)
axes[0].set(xlabel='Word count', title='Article title')
mean_title_length = news_df.Title_length.mean()
axes[0].axvline(mean_title_length, alpha=0.5, linestyle = '-.', c='black', label='Mean of title length')
axes[0].legend()

# Plotting distribution of article word count
sns.histplot(x='Raw_article_length', data=news_df, ax=axes[1], color='tab:orange', alpha=0.5)
axes[1].set(xlabel='Word count (in thousands)', title='Raw article text')
mean_article_length = news_df.Raw_article_length.mean()
axes[1].axvline(mean_article_length, alpha=0.5, linestyle = '--', c='black', label='Mean of main text length')
axes[1].legend()

# Global setup
plt.tight_layout()
plt.show()
plt.clf()
```

```{python}
article_length_summary = news_df[['Title_length', 'Raw_article_length']].describe()
```

```{r}
knitr::kable(py$article_length_summary, col.names = c("Title", "Raw main text"), caption="Summary statistics of the word counts of the news articles' titles and main texts")
```

## Sentiment analysis[^1]

[^1]: The code for implementing all the steps in this section will be available in the appendix.

### Preprocessing

After making sense of the dataset with EDA, it is time to build the sentiment analysis model to see whether the political affiliation of news media outlets is associated with the polarities of the news articles after controlling for other variables. But first there are some preprocessing steps to be done so that the data are transformed into suitable formats as inputs for machine learning models. For starters, columns of the metadata should be excluded for being the inputs of the models. Note that I have also removed the `Newspaper` column since H~1~ is more interested in whether newspaper outlets of the pro-Beijing camp *as a whole* may hold more negative attitudes towards asylum seekers in Hong Kong vis-a-vis media outlets with other political stances.

```{python dropping_metadata_columns}
metadata_columns = ["Index", "Date", "Category", "Page_number", "Newspaper"]
news_df.drop(columns=metadata_columns, inplace=True)
print(f"The metadata columns removed from the dataset are: {metadata_columns}.")
```

Furthermore, categorical variables will need to be transformed via dummy encoding, meaning that they will be transformed into `n-1` variables, with `n` being the number of distinct values in each categorical variable. The category not included as a new column will then become the reference category. For `Political_camp`, the pro-Beijing camp will be the reference category because H~1~ is interested in how this camp compares to others regarding the sentiment of the news articles. As for `Month`, I will bin the months into four yearly quarters (`Quarter`), with `Q1` (i.e. January, February and March) being the reference category.

```{python dummy_encoding}
# Making pro-Beijing become the reference category
news_df["Political_camp"] = pd.Categorical(news_df["Political_camp"], categories=['Pro-Beijing', 'Neutral', 'Pro-democracy'])  

# Binning the months into four quarters
def quarter(x):
  if x <= 3:
    return "Q1"
  elif x <= 6:
    return "Q2"
  elif x <= 9:
    return "Q3"
  else:
    return "Q4"
news_df["Quarter"] = news_df.Month.apply(quarter)
news_df.drop(columns="Month", inplace=True)

# Dummy encoding the categorical columsn
news_df = pd.get_dummies(news_df, columns=["Political_camp", "Quarter"], drop_first=True)

```

Next step is to transform both the titles and main texts of the articles into term-document matrix. The first step will be to join the `Title` and `Text` columns together. Afterwards, I will add additional words into the dictionary and remove stop words as well as punctuation for better tokenisation.

```{python joining_title_and_text}
news_df["Article"] = news_df.Title.str.cat(news_df.Text, sep=" ")
title_series = news_df.Title  # For indexing each article in the term-document matrix later
news_df.drop(columns=["Title", "Text"], inplace=True)
```

```{python read_text_function}
def read_text(path):
    with open(path, 'r', encoding='utf-8') as file:
        text = file.readlines()
        text = [word.replace('\n', '') for word in text]
        return text
```

```{python adding_words_to_dict, include=FALSE}
hk_politics_words = read_text('Coding/HKPolDict-master/merged.txt')  # Words related to Hong Kong politics
asylum_seeker_words = read_text('Coding/Asylum_seeker_words.txt')  # Words related to asylum seekers in Hong Kong
for word in chain(hk_politics_words, asylum_seeker_words):
  jieba.add_word(word)
```

```{python stop_word_list}
stop_words_cantonese = read_text('Coding/text_cleaning/cantonese_stopwords.txt')
punctuations = [punc for punc in zhon.hanzi.punctuation]
stop_words_full = list(set(word for word in chain(stop_words_cantonese, punctuations)))  # Concatenating the two lists of stop words and punctuation
```

Since the dataset has only around 550 observations, it is quite likely that the model will overfit if I directly use the term-document matrix as the input which contain thousands of columns as features (with each column corresponding to a token). Consequently, I will use a dimensionality reduction technique called non-negative matrix factorisation (NMF) decomposes the original term-document matrix into two matrices. According to \@stevensExploringTopicCoherence2012 (p.953), the two matrices generated by NMF, named *W* and *H*,

```{python tokeniser_setup}
# tokenizer
def tokenize_zh(doc):
  return jieba.cut(doc)

# Preprocessor
def preprocessor_zh(doc):
  regex_punctuation = r"[\d+\s+\n\t]|[\s+\.\!\/_,$%^*(+\"\']+|[+——！，。？、~@#￥%……&*（）]+|[【】╮╯▽╰╭★→「」]+|[！，❤。～《》：（）【】「」？”“；：、]"
  return re.sub(regex_punctuation, "", doc)
```

```{python tfidf_nmf}
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.decomposition import NMF

# Creating the tfidf matrix
tfidf_vec = TfidfVectorizer(min_df = 0.02, preprocessor=preprocessor_zh, tokenizer=tokenize_zh, stop_words=stop_words_full)
tfidf_matrix = tfidf_vec.fit_transform(news_df.Article)

# Let's set n_components as 10 for nmf for now
nmf_10 = NMF(n_components=10, max_iter=500, random_state=1)
articles_nmf = nmf_10.fit_transform(tfidf_matrix)

# Using NMF to reduce dimensionality
# reconstruct_error = []
# for i in range(1, 16):
#   nmf = NMF(n_components=i, max_iter=500, random_state=1)
#   articles_nmf = nmf.fit_transform(tfidf_matrix)
#   reconstruct_error.append(nmf.reconstruction_err_)
# 
# sns.lineplot(x=np.arange(1, 16), y=reconstruct_error)
# plt.show()
# plt.clf()

```

In order to make the latent features generated by the NMF model more intuitive, I will inspect the words that are

```{python inspecting_topic_words}
def topic_words(model, vectorizer, top_n_words):
  vocabulary = vectorizer.get_feature_names()
  for idx, topic in enumerate(model.components_):
    print(f"\nTopic {idx + 1}: ")
    print(" ".join([vocabulary[i] for i in topic.argsort()[:-top_n_words - 1: -1]]))

topic_words(nmf_10, tfidf_vec, 20)
```

```{python final_dataframe_concatenation}

```

### Training the model

```{python train_and_test}
from sklearn.model_selection import train_test_split

```

```{python evaluation_metrics}
from sklearn.metrics import log_loss, f1_score
def evaluate_model():
```

```{python model_spec}
from sklearn.linear_model import LogisticRegressionCV
from sklearn.svm import SVC
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler
from sklearn.compose import ColumnTransformer



```

<!-- ```{python lda_perplexity_test} -->

<!-- lda_perplexity = {} -->

<!-- for i in range(1, 21): -->

<!--   lda_model = LatentDirichletAllocation(n_components=i, random_state=1, max_iter=50) -->

<!--   _ = lda_model.fit(lda_count_matrix) -->

<!--   lda_perplexity[f"Topic(s): {i}"] = lda_model.perplexity(lda_count_matrix) -->

<!-- ``` -->

<!-- ```{python perplexity_plot} -->

<!-- sns.lineplot(x=lda_perplexity.keys(), y=lda_perplexity.values()) -->

<!-- sns.scatterplot(x=lda_perplexity.keys(), y=lda_perplexity.values()) -->

<!-- plt.gca().set(title="Perplexity score of LDA by number of topics", xlabel='Number of topics', ylabel='perplexity score', xticklabels=np.arange(1,21)) -->

<!-- plt.tight_layout() -->

<!-- plt.show() -->

<!-- plt.clf() -->

<!-- ``` -->

<!-- ```{python nmf_beta_divergence_test} -->

<!-- nmf_tfidf = TfidfVectorizer(preprocessor=preprocessor_zh, -->

<!--                             tokenizer=tokenize_zh, -->

<!--                             stop_words=stop_words_full, -->

<!--                             min_df=0.02) -->

<!-- nmf_term_matrix = nmf_tfidf.fit_transform(news_text) -->

<!-- nmf_beta_divergence = {} -->

<!-- for i in range(1, 21): -->

<!--   nmf_model = NMF(n_components=i, max_iter=1000) -->

<!--   _ = nmf_model.fit(nmf_term_matrix) -->

<!--   nmf_beta_divergence[f"Topics(s): {i}"] = nmf_model.reconstruction_err_ -->

<!-- ``` -->

<!-- ```{python nmf_beta_plot} -->

<!-- sns.lineplot(x=nmf_beta_divergence.keys(), y=nmf_beta_divergence.values()) -->

<!-- sns.scatterplot(x=nmf_beta_divergence.keys(), y=nmf_beta_divergence.values()) -->

<!-- plt.gca().set(title='Beta divergence of NMF model by number of topics', xlabel='Number of topics', ylabel='Beta divergence score', xticklabels=np.arange(1,21)) -->

<!-- plt.tight_layout() -->

<!-- plt.show() -->

<!-- plt.clf() -->

<!-- ``` -->

<!--chapter:end:04-results.Rmd-->

---
title: Conclusion
author: Author
chapter: 5
in_context: False
knit: iheiddown::chapter_pdf
output: iheiddown::chapter_pdf
---

\setcounter{chapter}{4}

# Conclusion {#conclusion}

In conclusion, it is found that at least in 2019, the political camp of media outlets was associated with their attitudes towards asylum seekers in Hong Kong. Specifically,

## How might the instigation of the National Security Law affect the public discourse on asylum seekers in Hong Kong?

Just a year after the anti-extradition law protest had started and once again mobilised a huge section of Hong Kong's society to oppose , the HKSAR Government promulgated the National Security Law in July 2020 which

As mentioned before, the opposition's political influence has been greatly crippled since the promulgation of the National Security Law in July 2020. With the conclusion of the recent 2021 Legislative Council election after an overhaul of the electoral system which essentially gatekeeps candidacy only to the "patriots" [@lau2021], Hong Kong's legislature, once a political venue where opposition parties could access political resources and advocate alternative policy discourses,

Even in the media

Alternative media such as Apple Daily and Stand News were also forced to shut down in 2021 due to the

<!--chapter:end:05-conclusion.Rmd-->

# Appendix: The Echoes of the Code{-}

```{=latex}
\markboth{Appendix}{Appendix}
```

The goal of this appendix is to echo the code you used in your thesis for a greater sense of transparency and replicability of your research. 
Note that `ref.labels` can be set to any label. 
Hence, you can filter the code you want replicated in the appendix by setting labels to the desired code chunks in the various chapters. 
See [this excellent resource](https://bookdown.org/yihui/rmarkdown-cookbook/code-appendix.html) for more information.


This might be particularly useful when you perform model selection to output intermediary steps here instead of in the code to avoid cluttering your report. 

```{r get-labels, echo = FALSE}
labs <- knitr::all_labels()
labs <- setdiff(labs, c("setup", "get-labels"))
```

```{r all-code, ref.label=labs, eval=FALSE}
```

# Appendix: The Echoes of the Code redux{-}

Add as many appendices as you like.

<!--chapter:end:98-appendices.Rmd-->

`r if(!knitr:::is_latex_output()) '# References {-}'`

<!-- 
You don't really need to alter this chapter unless you want to change the name of the bibliography section 
to 'Bibliography', 'Literature', or something similar.
Any references included in the chapters will be printed here automatically.
-->

<!-- This quiet code chunk serves as a custom style for the reference section and adds a custom header. Don't change it unless you really want to. -->
```{=latex}
\markboth{References}{References}
```

<!--chapter:end:99-references.Rmd-->

