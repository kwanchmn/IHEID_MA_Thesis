---
title: Results
author: Chin Man KWAN
chapter: 4
in_context: True
knit: iheiddown::chapter_pdf
output: iheiddown::chapter_pdf
---

\setcounter{chapter}{3}

```{r python_setup}
library(reticulate)
```

```{python results_modules}
import warnings
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import jieba
import zhon.hanzi
import re
from itertools import chain

pd.set_option('display.max_columns', None)
sns.set_theme(context='paper')
warnings.filterwarnings(category=FutureWarning, action = 'ignore')
```

```{python import_dataset}
news_df = pd.read_csv("Coding/asylum_seekers_articles_final.csv")
num_of_articles = news_df.shape[0]
num_of_newspapers = news_df.Newspaper.nunique()
```

# Results

After searching for articles which contain at least one of the keywords and are relevant to asylum seekers residing in Hong Kong, there were in total `r py$num_of_articles` articles published in 2019 by `r py$num_of_newspapers` newspapers. In this section, I will first explore the data set preliminarily, and then move onto sentiment analysis with machine learning models to find out whether the political camp of media outlets is associated with the polarity of the news articles towards asylum seekers.

## Exploratory data analysis (EDA)

### How does the number of news articles vary by political camps and month?

Starting with the number of articles by media outlets as shown in the left plot of figure 4.1, consistent with the study by @ngFramingIssueAsylum2019, Oriental Daily News continues to be the media outlet covering the most frequently on asylum seekers with 384 (or `r round(384/557 * 100, 2)`%) articles throughout 2019. By contrast, the second-most frequent publisher *Sing Tao Daily* only had 45 entries (or `r round(45/557 * 100, 2)`%) of the total number of articles published. Each of the other newspaper outlets only constituted to a small portion of news articles about non-refoulement claimants in 2019. Therefore, the issue of asylum seekers in Hong Kong still appeared to be the most salient for Oriental Daily News by 2019, evidenced by its unmatched volume of articles related to this topic vis-a-vis other media outlets.

On a higher level of political stance, the right plot of figure 4.1 indicates that largely due to the huge volume of articles by Oriental Daily News, the pro-Beijing camp dominated the coverage of asylum seekers in Hong Kong in 2019. Meanwhile, both neutral and pro-democracy newspaper outlets published similar amounts of articles throughout 2019, and both camps constituted to small proportions of the share of articles during the year. Even if we omitted the sheer volume of articles published by Oriental Daily News, the pro-Beijing media would still have `r 557-384` articles published altogether which was still considerably more than the quantity of articles by neutral and pro-democracy media outlets.

```{python, fig.cap="News articles on asylum seekers in 2019 by news outlet (left) and political camp (right)"}
fig, axes = plt.subplots(1,2)

# Plotting by-outlet amount of articles
articles_outlet = news_df.value_counts('Newspaper')
sns.barplot(x=articles_outlet, y=articles_outlet.index, ax=axes[0])
axes[0].set(xlabel='Number of articles (in base-10 log scale)', ylabel='', title='By newspaper', xscale="log")
for idx, value in enumerate(articles_outlet):
    axes[0].text(value + 5, idx + 0.2, value)

# Plotting by-camp amount of articles
sns.countplot(x='Political_camp', data=news_df, ax=axes[1])    
axes[1].set(xlabel='', ylabel='Number of articles', title='By political camp')
count_by_political_camp = news_df['Political_camp'].value_counts(sort=False)
for idx, value in enumerate(count_by_political_camp):
  axes[1].text(idx - 0.1, value + 5, value)
axes[1].set_xticklabels(['Pro-Beijing', 'Neutral', 'Pro-democracy'], rotation=60)
plt.tight_layout()
plt.show()
plt.clf()
```

Lastly, it will also be intriguing to see how the number of articles might vary by month in 2019. As noted before, the anti-extradition law protest lasted mostly from June to November when numerous large-scale clashes between protesters and the police occurred. From figure 4.2, it appears that coincidentally, there were the fewest amounts of articles about asylum seekers published between August and November when some of the most intense clashes (notably the *siege* of the Hong Kong Polytechnic University in November 2019) took place. Although investigating whether the number of news articles about asylum seekers may be correlated with that about the anti-extradition law protest would be out of the scope of this paper, this could be a research question to be pursued in another occasion.

```{python, fig.cap="Temporal patterns of the publication of news articles about asylum seekers in Hong Kong in 2019"}
articles_by_month = news_df.Month.value_counts(sort=False)
ax = sns.lineplot(x=articles_by_month.index, y=articles_by_month, color='tab:orange')
ax.set(xlabel='Month', ylabel='Number of articles', xticks=np.arange(1,13))
plt.tight_layout()
plt.show()
plt.clf()
```

In short, the majority of news articles about non-refoulement claimants in Hong Kong in 2019 were published by pro-Beijing media outlets, of which a huge proportion was from Oriental Daily News. Moreover, the number of articles by month was the lowest from August to November when the anti-extradition law witnessed some of the most large-scale and intense clashes.

### Polarities of the news articles

According to table 4.1, the polarity of the news articles about asylum seekers in Hong Kong in 2019 tilted towards negative, since only around 4.3% and 23.5% of articles respectively depicted asylum seekers positively and neutrally. The fact that the sentiment of the news articles in 2019 was skewed towards negativity implies that I will need to take class imbalance into account for modelling later. Political-camp-wise, pro-Beijing media outlets had over 70% of its articles depicting asylum seekers in Hong Kong in negative lights, whereas neutral and pro-democracy media outlets had their reportage evenly spread between neutral and positive articles (albeit they altogether constituted to only a small proportion of the total number of articles in 2019). While H~1~ shall be tested formally with machine learning models after including other control variables later, preliminary evidence suggests that the polarities of the news articles vary with the political camp that the outlets belong to.

```{python}
sentiment_camp = pd.crosstab(news_df.Political_camp, news_df.Sentiment, margins=True)
sentiment_camp.columns = ["Negative", "Neutral", "Positive", "All"]
```

```{r}
knitr::kable(py$sentiment_camp, digits = 4, caption="Polarities of the news articles on asylum seekers in Hong Kong in 2019")
```

### Presence of racial labels

Given the majority of asylum seekers in Hong Kong being non-ethnic Chinese, it will also be worth glimpsing whether the presence of racial labels for describing asylum seekers is associated with the sentiment of the news articles. Judging from figure 4.3 preliminarily, however, it appears that the patterns of the polarities are quite similar whether news articles contain racial labels or not, namely, most of the articles framed non-refoulement claimants negatively, and then some reported on events about this group of population neutrally, and finally only a small amount of articles were favourable towards asylum seekers residing in the city. In any case, the machine learning model can add the presence of racial labels as a control variable to test this potential association more formally later.

```{python}
ax = sns.countplot(x="Racial_label", hue="Sentiment", data=news_df)
ax.set(xlabel="Presence of racial labels", ylabel="Number of articles")
ax.set_xticklabels(["No", "Yes"])
plt.show()
plt.clf()
```

### Character lengths of news articles and titles

Lastly, let's look at the distribution of the character lengths of the titles and main texts of the news articles. According to figure 4.4 and table 4.2, it appears that both the title and main text lengths have right-skewed distributions. In other words, while most of the news articles on asylum seekers in Hong Kong in 2019 had relatively short titles and/or main texts, a few of them were considerably more verbose than the rest of the articles.

```{python, fig.cap="Distributions of the word counts of the articles' titles (left) and main texts (right)"}
fig, axes = plt.subplots(1, 2)

# Plotting distribution of title word count
sns.histplot(x='Title_length', data=news_df, ax=axes[0], color='tab:blue', alpha=0.5)
axes[0].set(xlabel='Word count', title='Article title')
mean_title_length = news_df.Title_length.mean()
axes[0].axvline(mean_title_length, alpha=0.5, linestyle = '-.', c='black', label='Mean of title length')
axes[0].legend()

# Plotting distribution of article word count
sns.histplot(x='Raw_article_length', data=news_df, ax=axes[1], color='tab:orange', alpha=0.5)
axes[1].set(xlabel='Word count (in thousands)', title='Raw article text')
mean_article_length = news_df.Raw_article_length.mean()
axes[1].axvline(mean_article_length, alpha=0.5, linestyle = '--', c='black', label='Mean of main text length')
axes[1].legend()

# Global setup
plt.tight_layout()
plt.show()
plt.clf()
```

```{python}
article_length_summary = news_df[['Title_length', 'Raw_article_length']].describe()
```

```{r}
knitr::kable(py$article_length_summary, col.names = c("Title", "Raw main text"), caption="Summary statistics of the word counts of the news articles' titles and main texts")
```

## Sentiment analysis[^1]

[^1]: The code for implementing all the steps in this section will be available in the appendix.

### Preprocessing

After making sense of the dataset with EDA, it is time to build the sentiment analysis model to see whether the political affiliation of news media outlets is associated with the polarities of the news articles after controlling for other variables. But first there are some preprocessing steps to be done so that the data are transformed into suitable formats as inputs for machine learning models. For starters, columns of the metadata should be excluded for being the inputs of the models. Note that I have also removed the `Newspaper` column since H~1~ is more interested in whether newspaper outlets of the pro-Beijing camp *as a whole* may hold more negative attitudes towards asylum seekers in Hong Kong vis-a-vis media outlets with other political stances.

```{python dropping_metadata_columns}
metadata_columns = ["Index", "Date", "Category", "Page_number", "Newspaper"]
news_df.drop(columns=metadata_columns, inplace=True)
print(f"The metadata columns removed from the dataset are: {metadata_columns}.")
```

Furthermore, I will need to transform the `Political_camp` and `Month` features with dummy encoding, meaning that these two categorical variables will be transformed into `n-1` variables, with `n` being the number of distinct values in each categorical variable. The unique category not included as a new column will then become the reference category. For `Political_camp`, the pro-Beijing camp will be the reference category because H~1~ is interested in how this camp compares to others regarding the sentiment of the news articles. As for `Month`, January will be the reference category.

```{python dummy_encoding}
news_df["Political_camp"] = pd.Categorical(news_df["Political_camp"], categories=['Pro-Beijing', 'Neutral', 'Pro-democracy'])  # So that pro-Beijing becomes the reference category
news_df = pd.get_dummies(news_df, columns=["Political_camp", "Month"], drop_first=True)
```

Next step is to transform both the titles and main texts of the articles into term-document matrix. The first step will be to join the `Title` and `Text` columns together. Afterwards, I will add additional words into the dictionary and remove stop words as well as punctuation for better tokenisation.

```{python joining_title_and_text}
news_df["Article"] = news_df.Title.str.cat(news_df.Text, sep=" ")
news_df.set_index("Title", inplace=True)  # For indexing each article in the term-document matrix later
news_df.drop(columns=["Text"], inplace=True)
```

```{python read_text_function}
def read_text(path):
    with open(path, 'r', encoding='utf-8') as file:
        text = file.readlines()
        text = [word.replace('\n', '') for word in text]
        return text
```

```{python adding_words_to_dict, include=FALSE}
hk_politics_words = read_text('Coding/HKPolDict-master/merged.txt')  # Words related to Hong Kong politics
asylum_seeker_words = read_text('Coding/Asylum_seeker_words.txt')  # Words related to asylum seekers in Hong Kong
for word in chain(hk_politics_words, asylum_seeker_words):
  jieba.add_word(word)
```

```{python stop_word_list}
stop_words_cantonese = read_text('Coding/text_cleaning/cantonese_stopwords.txt')
punctuations = [punc for punc in zhon.hanzi.punctuation]
stop_words_full = list(set(word for word in chain(stop_words_cantonese, punctuations)))  # Concatenating the two lists of stop words and punctuation
```

Since the dataset has only around 550 observations, it is quite likely that the model will overfit if I directly use the term-document matrix as the input which contain thousands of columns as features (with each column corresponding to a token). Consequently, I will use a dimensionality reduction technique called non-negative matrix factorisation (NMF) which transforms

```{python tokeniser_setup}
# tokenizer
def tokenize_zh(doc):
  return jieba.cut(doc)

# Preprocessor
def preprocessor_zh(doc):
  regex_punctuation = r"[\d+\s+\n\t]|[\s+\.\!\/_,$%^*(+\"\']+|[+——！，。？、~@#￥%……&*（）]+|[【】╮╯▽╰╭★→「」]+|[！，❤。～《》：（）【】「」？”“；：、]"
  return re.sub(regex_punctuation, "", doc)
```

```{python}
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.decomposition import NMF

# Creating the 
```

### Training the model

<!-- ```{python lda_perplexity_test} -->

<!-- lda_perplexity = {} -->

<!-- for i in range(1, 21): -->

<!--   lda_model = LatentDirichletAllocation(n_components=i, random_state=1, max_iter=50) -->

<!--   _ = lda_model.fit(lda_count_matrix) -->

<!--   lda_perplexity[f"Topic(s): {i}"] = lda_model.perplexity(lda_count_matrix) -->

<!-- ``` -->

<!-- ```{python perplexity_plot} -->

<!-- sns.lineplot(x=lda_perplexity.keys(), y=lda_perplexity.values()) -->

<!-- sns.scatterplot(x=lda_perplexity.keys(), y=lda_perplexity.values()) -->

<!-- plt.gca().set(title="Perplexity score of LDA by number of topics", xlabel='Number of topics', ylabel='perplexity score', xticklabels=np.arange(1,21)) -->

<!-- plt.tight_layout() -->

<!-- plt.show() -->

<!-- plt.clf() -->

<!-- ``` -->

<!-- ```{python nmf_beta_divergence_test} -->

<!-- nmf_tfidf = TfidfVectorizer(preprocessor=preprocessor_zh, -->

<!--                             tokenizer=tokenize_zh, -->

<!--                             stop_words=stop_words_full, -->

<!--                             min_df=0.02) -->

<!-- nmf_term_matrix = nmf_tfidf.fit_transform(news_text) -->

<!-- nmf_beta_divergence = {} -->

<!-- for i in range(1, 21): -->

<!--   nmf_model = NMF(n_components=i, max_iter=1000) -->

<!--   _ = nmf_model.fit(nmf_term_matrix) -->

<!--   nmf_beta_divergence[f"Topics(s): {i}"] = nmf_model.reconstruction_err_ -->

<!-- ``` -->

<!-- ```{python nmf_beta_plot} -->

<!-- sns.lineplot(x=nmf_beta_divergence.keys(), y=nmf_beta_divergence.values()) -->

<!-- sns.scatterplot(x=nmf_beta_divergence.keys(), y=nmf_beta_divergence.values()) -->

<!-- plt.gca().set(title='Beta divergence of NMF model by number of topics', xlabel='Number of topics', ylabel='Beta divergence score', xticklabels=np.arange(1,21)) -->

<!-- plt.tight_layout() -->

<!-- plt.show() -->

<!-- plt.clf() -->

<!-- ``` -->

```{python function_extract_topic_words}
def topic_words(model, vectorizer, top_n_words):
  vocabulary = vectorizer.get_feature_names()
  for idx, topic in enumerate(model.components_):
    print(f"\nTopic {idx + 1}: ")
    print(" ".join([vocabulary[i] for i in topic.argsort()[:-top_n_words - 1: -1]]))
```

```{python lda_topics_words}
lda_topics = LatentDirichletAllocation(n_components=10, random_state=1, max_iter=100)
_ = lda_topics.fit(lda_count_matrix)
topic_words(lda_topics, lda_count, 20)
```
