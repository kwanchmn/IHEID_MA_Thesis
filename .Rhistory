stop_words_full = list(set(word for word in chain(stop_words_cantonese, punctuations)))
# tokenizer
def tokenize_zh(doc):
return jieba.cut(doc)
# Preprocessor
def preprocessor_zh(doc):
regex_punctuation = r"[\d+\s+\n\t]|[\s+\.\!\/_,$%^*(+\"\']+|[+——！，。？、~@#￥%……&*（）]+|[【】╮╯▽╰╭★→「」]+|[！，❤。～《》：（）【】「」？”“；：、]"
return re.sub(regex_punctuation, "", doc)
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.decomposition import NMF
# Creating the tfidf matrix from the training set
tfidf_vec = TfidfVectorizer(min_df = 0.02,  # Each token must appear in at least 2% of the documents
preprocessor=preprocessor_zh,
tokenizer=tokenize_zh,
stop_words=stop_words_full)
X_train_tfidf = tfidf_vec.fit_transform(X_train.Article)
# Plotting the reconstruction error according to the number of latent topics
reconstruct_error = []
for i in range(1, 21):
nmf = NMF(n_components=i, max_iter=500, random_state=1)
articles_nmf = nmf.fit(X_train_tfidf)
reconstruct_error.append(nmf.reconstruction_err_)
ax = sns.lineplot(x=np.arange(1, 21), y=reconstruct_error)
ax.set(xlabel="Number of latent topics", ylabel="Reconstruction error", xticks=np.arange(1, 21))
plt.show()
plt.clf()
# Let's set n_components as 10 for nmf
nmf_10 = NMF(n_components=10, max_iter=500, random_state=1)
X_train_nmf = nmf_10.fit_transform(X_train_tfidf)
# Defining a function to extract the most prominent words in each topic
def topic_words(model, vectorizer, top_n_words):
vocabulary = vectorizer.get_feature_names()
for idx, topic in enumerate(model.components_):
print(f"\nTopic {idx + 1}: ")
print(" ".join([vocabulary[i] for i in topic.argsort()[:-top_n_words - 1: -1]]))
# Extracting the 30 most prominent words in each topic
_ = topic_words(nmf_10, tfidf_vec, 30)  # the output is shown in figure 4.6
quit
knitr::include_graphics("figures/nmf_topic_words_list.png")
citr:::insert_citation()
reticulate::repl_python()
# Defining a function to extract the most prominent words in each topic
def topic_words(model, vectorizer, top_n_words):
vocabulary = vectorizer.get_feature_names()
for idx, topic in enumerate(model.components_):
print(f"\nTopic {idx + 1}: ")
print(" ".join([vocabulary[i] for i in topic.argsort()[:-top_n_words - 1: -1]]))
# Extracting the 30 most prominent words in each topic
_ = topic_words(nmf_10, tfidf_vec, 30)  # the output is shown in figure 4.6
# Naming the latent topics more precisely
topics_list = ["Crimes", "Non-refoulement legal procedure", "Illegal labours", "Illegal gambling", "Drugs", "Illegal immigration", "Murder", "Robbery", "South Asian settlements", "Problem and solution"]
# Concatenating the NMF DataFrame for the training set
X_train_nmf_df = pd.DataFrame(X_train_nmf, index=X_train.index, columns=topics_list)
X_train_final = pd.concat([X_train, X_train_nmf_df], axis=1)
X_train_final.drop(columns="Article", inplace=True)
# Concatenating the NMF DataFrame for the validation set
X_test_tfidf = tfidf_vec.transform(X_test.Article)
X_test_nmf = nmf_10.transform(X_test_tfidf)
X_test_nmf_df = pd.DataFrame(X_test_nmf, index=X_test.index, columns=topics_list)
X_test_final = pd.concat([X_test, X_test_nmf_df], axis=1)
X_test_final.drop(columns="Article", inplace=True)
quit
library(reticulate)
reticulate::repl_python()
import warnings
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import jieba
import zhon.hanzi
import re
from itertools import chain
# Setting options for display and random seed
np.random.seed(1)
pd.set_option('display.max_columns', None)
sns.set_theme(context='paper')
warnings.filterwarnings(category=FutureWarning, action = 'ignore')
warnings.filterwarnings(category=UserWarning, action = 'ignore')
news_df = pd.read_csv("data/asylum_seekers_articles_final.csv")
num_of_articles = news_df.shape[0]
num_of_newspapers = news_df.Newspaper.nunique()
articles_by_month = news_df.Month.value_counts(sort=False)
ax = sns.lineplot(x=articles_by_month.index, y=articles_by_month, color='tab:orange')
ax.set(xlabel='Month', ylabel='Number of articles', xticks=np.arange(1, 13))
plt.tight_layout()
plt.show()
plt.clf()
sentiment_camp = pd.crosstab(news_df.Political_camp, news_df.Sentiment, margins=True)
sentiment_camp.columns = ["Negative", "Neutral", "Positive", "All"]
quit
knitr::kable(py$sentiment_camp, digits = 4, caption="Polarities of the news articles on asylum seekers in Hong Kong in 2019")
reticulate::repl_python()
ax = sns.countplot(x="Racial_label", hue="Sentiment", data=news_df)
ax.set(xlabel="Presence of racial labels", ylabel="Number of articles")
ax.set_xticklabels(["No", "Yes"])
ax.legend(labels=["Negative", "Neutral", "Positive"], title="Sentiment")
plt.show()
plt.clf()
fig, axes = plt.subplots(1, 2)
# Plotting distribution of title word count
sns.histplot(x='Title_length', data=news_df, ax=axes[0], color='tab:blue', alpha=0.5)
axes[0].set(xlabel='Word count', title='Article title')
mean_title_length = news_df.Title_length.mean()
axes[0].axvline(mean_title_length, alpha=0.5, linestyle = '-.', c='black', label='Mean of title length')
axes[0].legend()
# Plotting distribution of article word count
sns.histplot(x='Raw_article_length', data=news_df, ax=axes[1], color='tab:orange', alpha=0.5)
axes[1].set(xlabel='Word count (in thousands)', title='Raw article text')
mean_article_length = news_df.Raw_article_length.mean()
axes[1].axvline(mean_article_length, alpha=0.5, linestyle = '--', c='black', label='Mean of main text length')
axes[1].legend()
# Global setup
plt.tight_layout()
plt.show()
plt.clf()
article_length_summary = news_df[['Title_length', 'Raw_article_length']].describe()
quit
knitr::kable(py$article_length_summary, col.names = c("Title", "Raw main text"), caption="Summary statistics of the word counts of the news articles' titles and main texts")
reticulate::repl_python()
metadata_columns = ["Index", "Date", "Category", "Page_number", "Newspaper"]
news_df.drop(columns=metadata_columns, inplace=True)
# Making pro-Beijing become the reference category
news_df["Political_camp"] = pd.Categorical(news_df["Political_camp"], categories=['Pro-Beijing', 'Neutral', 'Pro-democracy'])
# Binning the months into four quarters
def quarter(x):
if x <= 3:
return "Q1"
elif x <= 6:
return "Q2"
elif x <= 9:
return "Q3"
else:
return "Q4"
news_df["Quarter"] = pd.Categorical(news_df.Month.apply(quarter))
news_df.drop(columns="Month", inplace=True)
# One-hot encoding
news_df = pd.get_dummies(news_df, columns=["Political_camp", "Quarter"])
from sklearn.model_selection import train_test_split
X = news_df.drop(columns="Sentiment")
y = news_df.Sentiment
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=1)
# Train set
X_train["Article"] = X_train.Title.str.cat(news_df.Text, sep=" ")
X_train.drop(columns=["Text", "Title"], inplace=True)
# Test set
X_test["Article"] = X_test.Title.str.cat(news_df.Text, sep=" ")
X_test.drop(columns=["Text", "Title"], inplace=True)
def read_text(path):
with open(path, 'r', encoding='utf-8') as file:
text = file.readlines()
text = [word.replace('\n', '') for word in text]
return text
hk_politics_words = read_text('Coding/HKPolDict-master/merged.txt')  # Words related to Hong Kong politics
asylum_seeker_words = read_text('Coding/Asylum_seeker_words.txt')  # Words related to asylum seekers in Hong Kong
for word in chain(hk_politics_words, asylum_seeker_words):
jieba.add_word(word)
stop_words_cantonese = read_text('Coding/text_cleaning/cantonese_stopwords.txt')
punctuations = [punc for punc in zhon.hanzi.punctuation]
stop_words_full = list(set(word for word in chain(stop_words_cantonese, punctuations)))
# tokenizer
def tokenize_zh(doc):
return jieba.cut(doc)
# Preprocessor
def preprocessor_zh(doc):
regex_punctuation = r"[\d+\s+\n\t]|[\s+\.\!\/_,$%^*(+\"\']+|[+——！，。？、~@#￥%……&*（）]+|[【】╮╯▽╰╭★→「」]+|[！，❤。～《》：（）【】「」？”“；：、]"
return re.sub(regex_punctuation, "", doc)
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.decomposition import NMF
# Creating the tfidf matrix from the training set
tfidf_vec = TfidfVectorizer(min_df = 0.02,  # Each token must appear in at least 2% of the documents
preprocessor=preprocessor_zh,
tokenizer=tokenize_zh,
stop_words=stop_words_full)
X_train_tfidf = tfidf_vec.fit_transform(X_train.Article)
# Plotting the reconstruction error according to the number of latent topics
reconstruct_error = []
for i in range(1, 21):
nmf = NMF(n_components=i, max_iter=500, random_state=1)
articles_nmf = nmf.fit(X_train_tfidf)
reconstruct_error.append(nmf.reconstruction_err_)
ax = sns.lineplot(x=np.arange(1, 21), y=reconstruct_error)
ax.set(xlabel="Number of latent topics", ylabel="Reconstruction error", xticks=np.arange(1, 21))
plt.show()
plt.clf()
# Let's set n_components as 10 for nmf
nmf_10 = NMF(n_components=10, max_iter=500, random_state=1)
X_train_nmf = nmf_10.fit_transform(X_train_tfidf)
# Defining a function to extract the most prominent words in each topic
def topic_words(model, vectorizer, top_n_words):
vocabulary = vectorizer.get_feature_names()
for idx, topic in enumerate(model.components_):
print(f"\nTopic {idx + 1}: ")
print(" ".join([vocabulary[i] for i in topic.argsort()[:-top_n_words - 1: -1]]))
# Extracting the 30 most prominent words in each topic
_ = topic_words(nmf_10, tfidf_vec, 30)  # the output is shown in figure 4.6
quit
knitr::include_graphics("figures/nmf_topic_words_list.png")
reticulate::repl_python()
# Naming the latent topics more precisely
topics_list = ["Crimes", "Non-refoulement legal procedure", "Illegal labours", "Illegal gambling", "Drugs", "Illegal immigration", "Murder", "Robbery", "South Asian settlements", "Problem and solution"]
# Concatenating the NMF DataFrame for the training set
X_train_nmf_df = pd.DataFrame(X_train_nmf, index=X_train.index, columns=topics_list)
X_train_final = pd.concat([X_train, X_train_nmf_df], axis=1)
X_train_final.drop(columns="Article", inplace=True)
# Concatenating the NMF DataFrame for the validation set
X_test_tfidf = tfidf_vec.transform(X_test.Article)
X_test_nmf = nmf_10.transform(X_test_tfidf)
X_test_nmf_df = pd.DataFrame(X_test_nmf, index=X_test.index, columns=topics_list)
X_test_final = pd.concat([X_test, X_test_nmf_df], axis=1)
X_test_final.drop(columns="Article", inplace=True)
from sklearn.model_selection import cross_val_score, StratifiedKFold
from sklearn.metrics import f1_score, make_scorer
# Defining the kfold strategy
five_fold_cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=1)
# Utility function for evaluating the model's performance in cross validation and test set in terms of log loss
def evaluate_model_f1(model, model_name: str, cv=five_fold_cv, X_train=X_train_final, X_test=X_test_final, y_train=y_train, y_test=y_test):
y_pred = model.predict(X_test)
cv_f1_score = np.mean(cross_val_score(model, X_train, y_train, cv=cv, scoring=make_scorer(f1_score, average="macro")))
test_f1_score = f1_score(y_test, y_pred, average="macro")
return {"5-fold cv f1 score": cv_f1_score, "Test set f1 score": test_f1_score}
from sklearn.preprocessing import StandardScaler
from sklearn.compose import ColumnTransformer
# Separating the columns for respective preprocessing steps
numeric_columns = [col for col in X_train_final.columns if X_train_final[col].dtype in ["int64", "float64"] and col != "Racial_label"]
# Preprocessor for linear models
stand_preprocessor = ColumnTransformer([("standardiser", StandardScaler(), numeric_columns)],
remainder='passthrough')
quit
knitr::kable(py$baseline_f1_score_df, caption = "Log loss on 5-fold cv and test set for the 4 baseline models")
library(reticulate)
reticulate::repl_python()
import warnings
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import jieba
import zhon.hanzi
import re
from itertools import chain
# Setting options for display and random seed
np.random.seed(1)
pd.set_option('display.max_columns', None)
sns.set_theme(context='paper')
warnings.filterwarnings(category=FutureWarning, action = 'ignore')
warnings.filterwarnings(category=UserWarning, action = 'ignore')
news_df = pd.read_csv("data/asylum_seekers_articles_final.csv")
num_of_articles = news_df.shape[0]
num_of_newspapers = news_df.Newspaper.nunique()
articles_by_month = news_df.Month.value_counts(sort=False)
ax = sns.lineplot(x=articles_by_month.index, y=articles_by_month, color='tab:orange')
ax.set(xlabel='Month', ylabel='Number of articles', xticks=np.arange(1, 13))
plt.tight_layout()
plt.show()
plt.clf()
sentiment_camp = pd.crosstab(news_df.Political_camp, news_df.Sentiment, margins=True)
sentiment_camp.columns = ["Negative", "Neutral", "Positive", "All"]
quit
knitr::kable(py$sentiment_camp, digits = 4, caption="Polarities of the news articles on asylum seekers in Hong Kong in 2019")
reticulate::repl_python()
ax = sns.countplot(x="Racial_label", hue="Sentiment", data=news_df)
ax.set(xlabel="Presence of racial labels", ylabel="Number of articles")
ax.set_xticklabels(["No", "Yes"])
ax.legend(labels=["Negative", "Neutral", "Positive"], title="Sentiment")
plt.show()
plt.clf()
fig, axes = plt.subplots(1, 2)
# Plotting distribution of title word count
sns.histplot(x='Title_length', data=news_df, ax=axes[0], color='tab:blue', alpha=0.5)
axes[0].set(xlabel='Word count', title='Article title')
mean_title_length = news_df.Title_length.mean()
axes[0].axvline(mean_title_length, alpha=0.5, linestyle = '-.', c='black', label='Mean of title length')
axes[0].legend()
# Plotting distribution of article word count
sns.histplot(x='Raw_article_length', data=news_df, ax=axes[1], color='tab:orange', alpha=0.5)
axes[1].set(xlabel='Word count (in thousands)', title='Raw article text')
mean_article_length = news_df.Raw_article_length.mean()
axes[1].axvline(mean_article_length, alpha=0.5, linestyle = '--', c='black', label='Mean of main text length')
axes[1].legend()
# Global setup
plt.tight_layout()
plt.show()
plt.clf()
article_length_summary = news_df[['Title_length', 'Raw_article_length']].describe()
quit
knitr::kable(py$article_length_summary, col.names = c("Title", "Raw main text"), caption="Summary statistics of the word counts of the news articles' titles and main texts")
reticulate::repl_python()
metadata_columns = ["Index", "Date", "Category", "Page_number", "Newspaper"]
news_df.drop(columns=metadata_columns, inplace=True)
# Making pro-Beijing become the reference category
news_df["Political_camp"] = pd.Categorical(news_df["Political_camp"], categories=['Pro-Beijing', 'Neutral', 'Pro-democracy'])
# Binning the months into four quarters
def quarter(x):
if x <= 3:
return "Q1"
elif x <= 6:
return "Q2"
elif x <= 9:
return "Q3"
else:
return "Q4"
news_df["Quarter"] = pd.Categorical(news_df.Month.apply(quarter))
news_df.drop(columns="Month", inplace=True)
# One-hot encoding
news_df = pd.get_dummies(news_df, columns=["Political_camp", "Quarter"])
from sklearn.model_selection import train_test_split
X = news_df.drop(columns="Sentiment")
y = news_df.Sentiment
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=1)
# Train set
X_train["Article"] = X_train.Title.str.cat(news_df.Text, sep=" ")
X_train.drop(columns=["Text", "Title"], inplace=True)
# Test set
X_test["Article"] = X_test.Title.str.cat(news_df.Text, sep=" ")
X_test.drop(columns=["Text", "Title"], inplace=True)
def read_text(path):
with open(path, 'r', encoding='utf-8') as file:
text = file.readlines()
text = [word.replace('\n', '') for word in text]
return text
hk_politics_words = read_text('Coding/HKPolDict-master/merged.txt')  # Words related to Hong Kong politics
asylum_seeker_words = read_text('Coding/Asylum_seeker_words.txt')  # Words related to asylum seekers in Hong Kong
for word in chain(hk_politics_words, asylum_seeker_words):
jieba.add_word(word)
stop_words_cantonese = read_text('Coding/text_cleaning/cantonese_stopwords.txt')
punctuations = [punc for punc in zhon.hanzi.punctuation]
stop_words_full = list(set(word for word in chain(stop_words_cantonese, punctuations)))
# tokenizer
def tokenize_zh(doc):
return jieba.cut(doc)
# Preprocessor
def preprocessor_zh(doc):
regex_punctuation = r"[\d+\s+\n\t]|[\s+\.\!\/_,$%^*(+\"\']+|[+——！，。？、~@#￥%……&*（）]+|[【】╮╯▽╰╭★→「」]+|[！，❤。～《》：（）【】「」？”“；：、]"
return re.sub(regex_punctuation, "", doc)
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.decomposition import NMF
# Creating the tfidf matrix from the training set
tfidf_vec = TfidfVectorizer(min_df = 0.02,  # Each token must appear in at least 2% of the documents
preprocessor=preprocessor_zh,
tokenizer=tokenize_zh,
stop_words=stop_words_full)
X_train_tfidf = tfidf_vec.fit_transform(X_train.Article)
# Plotting the reconstruction error according to the number of latent topics
reconstruct_error = []
for i in range(1, 21):
nmf = NMF(n_components=i, max_iter=500, random_state=1)
articles_nmf = nmf.fit(X_train_tfidf)
reconstruct_error.append(nmf.reconstruction_err_)
ax = sns.lineplot(x=np.arange(1, 21), y=reconstruct_error)
ax.set(xlabel="Number of latent topics", ylabel="Reconstruction error", xticks=np.arange(1, 21))
plt.show()
plt.clf()
# Let's set n_components as 10 for nmf
nmf_10 = NMF(n_components=10, max_iter=500, random_state=1)
X_train_nmf = nmf_10.fit_transform(X_train_tfidf)
# Defining a function to extract the most prominent words in each topic
def topic_words(model, vectorizer, top_n_words):
vocabulary = vectorizer.get_feature_names()
for idx, topic in enumerate(model.components_):
print(f"\nTopic {idx + 1}: ")
print(" ".join([vocabulary[i] for i in topic.argsort()[:-top_n_words - 1: -1]]))
# Extracting the 30 most prominent words in each topic
_ = topic_words(nmf_10, tfidf_vec, 30)  # the output is shown in figure 4.6
quit
knitr::include_graphics("figures/nmf_topic_words_list.png")
reticulate::repl_python()
# Naming the latent topics more precisely
topics_list = ["Crimes", "Non-refoulement legal procedure", "Illegal labours", "Illegal gambling", "Drugs", "Illegal immigration", "Murder", "Robbery", "South Asian settlements", "Problem and solution"]
# Concatenating the NMF DataFrame for the training set
X_train_nmf_df = pd.DataFrame(X_train_nmf, index=X_train.index, columns=topics_list)
X_train_final = pd.concat([X_train, X_train_nmf_df], axis=1)
X_train_final.drop(columns="Article", inplace=True)
# Concatenating the NMF DataFrame for the validation set
X_test_tfidf = tfidf_vec.transform(X_test.Article)
X_test_nmf = nmf_10.transform(X_test_tfidf)
X_test_nmf_df = pd.DataFrame(X_test_nmf, index=X_test.index, columns=topics_list)
X_test_final = pd.concat([X_test, X_test_nmf_df], axis=1)
X_test_final.drop(columns="Article", inplace=True)
from sklearn.model_selection import cross_val_score, StratifiedKFold
from sklearn.metrics import f1_score, make_scorer
# Defining the kfold strategy
five_fold_cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=1)
# Utility function for evaluating the model's performance in cross validation and test set in terms of log loss
def evaluate_model_f1(model, model_name: str, cv=five_fold_cv, X_train=X_train_final, X_test=X_test_final, y_train=y_train, y_test=y_test):
y_pred = model.predict(X_test)
cv_f1_score = np.mean(cross_val_score(model, X_train, y_train, cv=cv, scoring=make_scorer(f1_score, average="macro")))
test_f1_score = f1_score(y_test, y_pred, average="macro")
return {"5-fold cv f1 score": cv_f1_score, "Test set f1 score": test_f1_score}
from sklearn.preprocessing import StandardScaler
from sklearn.compose import ColumnTransformer
# Separating the columns for respective preprocessing steps
numeric_columns = [col for col in X_train_final.columns if X_train_final[col].dtype in ["int64", "float64"] and col != "Racial_label"]
# Preprocessor for linear models
stand_preprocessor = ColumnTransformer([("standardiser", StandardScaler(), numeric_columns)], remainder='passthrough')
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.ensemble import RandomForestClassifier
from sklearn.pipeline import Pipeline
from sklearn.utils.class_weight import compute_sample_weight
import xgboost as xgb
# Logistic regression pipeline
log_reg_baseline = Pipeline([("preprocess", stand_preprocessor),
("log_reg", LogisticRegression(random_state=1,
class_weight="balanced"))])
_ = log_reg_baseline.fit(X_train_final, y_train)
log_reg_base_result = evaluate_model_f1(log_reg_baseline, "baseline logistic regression")
# SVM pipeline
svm_baseline = Pipeline([("preprocess", stand_preprocessor),
("svm", SVC(probability=True, class_weight="balanced", random_state=1))])
_ = svm_baseline.fit(X_train_final, y_train)
svm_base_result = evaluate_model_f1(svm_baseline, "baseline support vector machine")
# Random forest pipeline
rf_baseline = RandomForestClassifier(class_weight="balanced", random_state=1, criterion="entropy")
_ = rf_baseline.fit(X_train_final, y_train)
rf_base_result = evaluate_model_f1(rf_baseline, "baseline random forest")
# xgboost pipeline
xgb_sample_weight = compute_sample_weight(class_weight="balanced", y=y_train)
xgboost_baseline = xgb.XGBClassifier(objective="multi:softmax",
eval_metric="mlogloss",
seed=1,
use_label_encoder=False)
_ = xgboost_baseline.fit(X_train_final,
y_train,
sample_weight=xgb_sample_weight,
eval_set=[(X_test_final, y_test)],
early_stopping_rounds=5,
verbose=0)
xgb_base_result = evaluate_model_f1(xgboost_baseline, "baseline XGBoost")
# Creating the DataFrame of the baseline results
baseline_f1_score_df = pd.DataFrame([log_reg_base_result, svm_base_result, rf_base_result, xgb_base_result], index=["Logistic regression", "SVM", "Random forest", "XGBoost classifier"])
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.ensemble import RandomForestClassifier
from sklearn.pipeline import Pipeline
from sklearn.utils.class_weight import compute_sample_weight
import xgboost as xgb
# Logistic regression pipeline
log_reg_baseline = Pipeline([("preprocess", stand_preprocessor),
("log_reg", LogisticRegression(random_state=1,
class_weight="balanced"))])
_ = log_reg_baseline.fit(X_train_final, y_train)
log_reg_base_result = evaluate_model_f1(log_reg_baseline, "baseline logistic regression")
# SVM pipeline
svm_baseline = Pipeline([("preprocess", stand_preprocessor),
("svm", SVC(probability=True, class_weight="balanced", random_state=1))])
_ = svm_baseline.fit(X_train_final, y_train)
svm_base_result = evaluate_model_f1(svm_baseline, "baseline support vector machine")
# Random forest pipeline
rf_baseline = RandomForestClassifier(class_weight="balanced", random_state=1, criterion="entropy")
_ = rf_baseline.fit(X_train_final, y_train)
rf_base_result = evaluate_model_f1(rf_baseline, "baseline random forest")
# xgboost pipeline
xgb_sample_weight = compute_sample_weight(class_weight="balanced", y=y_train)
xgboost_baseline = xgb.XGBClassifier(objective="multi:softmax",
eval_metric="mlogloss",
seed=1,
use_label_encoder=False)
_ = xgboost_baseline.fit(X_train_final,
y_train,
sample_weight=xgb_sample_weight,
eval_set=[(X_test_final, y_test)],
early_stopping_rounds=5,
verbose=0)
xgb_base_result = evaluate_model_f1(xgboost_baseline, "baseline XGBoost")
# Creating the DataFrame of the baseline results
baseline_f1_score_df = pd.DataFrame([log_reg_base_result, svm_base_result, rf_base_result, xgb_base_result], index=["Logistic regression", "SVM", "Random forest", "XGBoost classifier"])
quit
knitr::kable(py$baseline_f1_score_df, caption = "F1 scores on 5-fold cross validation and test set of the 4 baseline models")
reticulate::repl_python()
import pickle
from sklearn.metrics import classification_report
# Loading the tuned model
xgboost_tuned = pickle.load(open("models/xgb_clf_tuned_II.pkl", "rb"))
# Getting the performance of the tuned model
xgb_tuned_result = evaluate_model_f1(xgboost_tuned, "Tuned xgboost")
# Creating DataFrame to compare baseline and tuned macro f1 score
xgb_compare_df = pd.DataFrame([xgb_base_result, xgb_tuned_result], index=["Baseline", "Tuned"])
# Creating classification_report for the tuned model
xgb_tuned_clf_report = pd.DataFrame(classification_report(y_test, xgboost_tuned.predict(X_test_final), output_dict=True)).T
quit
knitr::kable(py$xgb_tuned_clf_report, digits=4, caption="Classification report on the f1 score of the tuned XGBoost model on the testing data")
knitr::kable(py$xgb_compare_df, digits=4, caption="Comparison of the f1 scores on 5-fold cross validation and test set between the baseline and tuned XGBoost models")
knitr::kable(py$baseline_f1_score_df, caption = "F1 scores on 5-fold cross validation and test data of the 4 baseline models")
knitr::kable(py$xgb_compare_df, digits=4, caption="Comparison of the f1 scores on 5-fold cross validation and test data between the baseline and tuned XGBoost models")
reticulate::repl_python()
# Setting up the shap values
import shap
xgb_explainer = shap.TreeExplainer(xgboost_tuned)
xgb_shap_values = xgb_explainer.shap_values(X_train_final)
# Getting the expected probability of each class in one-versus-all manner
expected_0, expected_1, expected_2 = xgb_explainer.expected_value
# Defining the summary plot function
def shap_summary_plot(class_label=None, **kwargs):
if class_label == None:
shap.summary_plot(xgb_shap_values, X_train_final, show=False, **kwargs)
else:
shap.summary_plot(xgb_shap_values[class_label], X_train_final, show=False, cmap="Greys", **kwargs)
plt.tight_layout()
plt.show()
plt.clf()
# Defining the dependence plot function
def shap_dependence_plot(feature_list, class_label, interaction_feature=None, **kwargs):
fig, axes = plt.subplots(2, 2)
for feature, ax in zip(feature_list, axes.ravel()):
shap.dependence_plot(feature, xgb_shap_values[class_label], X_train_final, ax=ax, title=feature, show=False, cmap="Greys", interaction_index=interaction_feature, **kwargs)
ax.set(xlabel="", ylabel="")
fig.supxlabel("Feature values")
fig.supylabel("SHAP values")
plt.tight_layout()
plt.show()
plt.clf()
