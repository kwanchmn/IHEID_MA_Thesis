---
title: Results
author: Chin Man KWAN
chapter: 4
in_context: True
knit: iheiddown::chapter_pdf
output: iheiddown::chapter_pdf
---

\setcounter{chapter}{3}

```{r python_setup}
library(reticulate)
```

```{python results_modules}
import warnings
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import jieba
import zhon.hanzi
import re
from itertools import chain
pd.set_option('display.max_columns', None)
sns.set_theme(context='paper')
warnings.filterwarnings(category=FutureWarning, action = 'ignore')
```

```{python import_dataset}
all_news_articles = pd.read_csv('Coding/all_news_articles.csv')
num_of_articles = all_news_articles.shape[0]
num_of_newspapers = all_news_articles.Newspaper.nunique()
```

# Results

After searching for articles which contain at least one of the keywords and are relevant to asylum seekers residing in Hong Kong, there were in total `r py$num_of_articles` articles published in 2019 by `r py$num_of_newspapers` newspapers. In this section, I will first explore the data set preliminarily, and then move onto topic modelling with LDA and NMF before choosing which model performs better in finding out coherent topics.

## Exploratory data analysis (EDA)

### Number of news articles

Let us first explore how the number of publications of news articles about asylum seekers might differ by newspaper outlets and month. Starting with the number of articles by media outlets in figure 4.1, consistent with the study by @ngFramingIssueAsylum2019, Oriental Daily News continues to be the media outlet covering the most frequently on asylum seekers with 545 (or `r round(545/1115*100, 2)`%) articles throughout 2019. The second-most frequent publisher Sing Tao Daily by contrast only had 132 entries which was `r round(132/545 * 100, 2)`% of Oriental Daily News. Apple Daily, the then-most prominent pro-democracy printed media, ranked third in the coverage of non-refoulement claimants in 2019. If we look at the number of news articles by political camp, then once again the pattern in 2019 echoes with that observed by @ngFramingIssueAsylum2019 back in 2015-16, namely, the coverage of non-refoulement claimants in Hong Kong were mainly done by pro-Beijing news media, as almost 85% of the news articles on this issue came from pro-Beijing affiliated media outlets. (Question: Should I just lump pro-democracy and neutral outlets into the 'non-pro-Beijing' category then?)

```{python, fig.cap="News articles on asylum seekers in 2019 by news outlet (left) and political camp (right)"}
fig, axes = plt.subplots(1,2)

# Plotting by-outlet amount of articles
articles_outlet = all_news_articles.value_counts('Newspaper')
sns.barplot(x=articles_outlet, y=articles_outlet.index, ax=axes[0])
axes[0].set(xlabel='Number of articles', ylabel='', title='By news outlet')
for idx, value in enumerate(articles_outlet):
    axes[0].text(value + 5, idx + 0.2, value)

# Plotting by-camp amount of articles
sns.countplot(x='Political_camp', data=all_news_articles, ax=axes[1])    
axes[1].set(xlabel='', ylabel='Number of articles', title='By political camp')
count_by_political_camp = all_news_articles['Political_camp'].value_counts(sort=False)
for idx, value in enumerate(count_by_political_camp):
  axes[1].text(idx - 0.1, value + 5, value)
axes[1].set_xticklabels(['Pro-Beijing', 'Neutral', 'Pro-democracy'], rotation=60)
plt.tight_layout()
plt.show()
plt.clf()
```

Next will be to look at how the number of articles might vary temporally in figure 4.2. Month-wise, January was the month with the highest amount of asylum-seeker-related articles published in 2019. Since then, the coverage of asylum seekers decreased from February to April until it bounced back in May. As the Anti-Extradition Law Protest started in June, the number of articles published first rose slightly in July and then dropped drastically in the next two months, until the figure bounced back again in October. If we look at the number of news articles published before and during the anti-extradition law protest since June, then the two periods had similar amounts of publications.

```{python, fig.cap="Temporal patterns of the publication of news articles about asylum seekers in Hong Kong in 2019"}
fig, axes = plt.subplots(1,2)

# Plotting by-month articles
articles_by_month = all_news_articles.Month.value_counts(sort=False)
sns.lineplot(x=articles_by_month.index, y=articles_by_month, color='tab:orange', ax=axes[0])
axes[0].set(xlabel='Month', ylabel='Number of articles', title='By month', xticks=np.arange(1,13))

# Plotting amount of articles before and during the anti-extradition law protest
sns.countplot(x='Protest', data=all_news_articles, ax=axes[1])
axes[1].set(xlabel='', ylabel='Number of articles', title='By before and during protest')
axes[1].set_xticklabels(['Before (Jan-May)', 'During (June-Dec)'])
plt.tight_layout()
plt.show()
plt.clf()
```

### Character lengths of news articles and titles

```{python, fig.cap="Distributions of the word counts of the articles' titles (left) and main texts (right)"}
fig, axes = plt.subplots(1, 2)

# Plotting distribution of title word count
sns.histplot(x='Title_length', data=all_news_articles, ax=axes[0], color='tab:blue', alpha=0.5)
axes[0].set(xlabel='Word count', title='Article title')
mean_title_length = all_news_articles.Title_length.mean()
axes[0].axvline(mean_title_length, alpha=0.5, linestyle = '-.', c='black', label='Mean of title length')
axes[0].legend()

# Plotting distribution of article word count
sns.histplot(x='Raw_article_length', data=all_news_articles, ax=axes[1], color='tab:orange', alpha=0.5)
axes[1].set(xlabel='Word count (in thousands)', title='Raw article text', xticks=np.arange(0, 17e3, 2e3), xticklabels=np.arange(0, 17, 2))
mean_article_length = all_news_articles.Raw_article_length.mean()
axes[1].axvline(mean_article_length, alpha=0.5, linestyle = '--', c='black', label='Mean of main text length')
axes[1].legend()

# Global setup
plt.tight_layout()
plt.show()
plt.clf()
```

The next interesting pattern to explore will be the distribution of the word count of each document, which are shown on the histograms in figure 4.3. On the left plot, a large proportion of the news articles have their title's word counts ranging from 10 to 20 Chinese characters long, but there are also articles whose titles have more than 50 characters. On the right of figure 4.3 is the distribution of the word counts of the main texts in the news articles prior to preprocessing. Most of the news articles in the dataset were less than 2000 characters long, but there was one with around 16000 words in the main text as well. Table 4.1 contains the detailed summary statistics of the lengths of the news articles' titles and main texts.

```{python}
article_length_summary = all_news_articles[['Title_length', 'Raw_article_length']].describe()
```

```{r}
knitr::kable(py$article_length_summary, col.names = c("Title", "Raw main text"), caption="Summary statistics of the word counts of the news articles' titles and main texts")
```

## Topic modelling

After performing some EDA on the raw dataset, it is time to perform the preprocessing steps required for topic modelling with LDA and NMF. 

```{python read_text_function}
def read_text(path, output=None):
  with open(path, 'r', encoding='utf-8') as file:
    text = file.readlines()
    text = [word.replace('\n', '') for word in text]
    return text
```

```{python adding_words_to_dict, include=FALSE}
hk_politics_words = read_text('Coding/HKPolDict-master/merged.txt') # Hong Kong politics
asylum_seeker_words = read_text('Coding/Asylum_seeker_words.txt') # Asylum seekers
for word in chain(hk_politics_words, asylum_seeker_words):
  jieba.add_word(word)
```

```{python text_cleaning}
# Creating the stop word list
stop_words_cantonese = read_text('Coding/text_cleaning/cantonese_stopwords.txt')
punctuations = [punc for punc in zhon.hanzi.punctuation]
stop_words_full = list(set(word for word in chain(stop_words_cantonese, punctuations)))
```

```{python checking_parentheses_content, eval=FALSE}
parents = all_news_articles['Text'].apply(lambda row: re.findall(r'(\（.+?\）)', row))
parent_word_list = []
for line in parents:
  for word in line:
    parent_word_list.append(word)
parent_word_list = sorted(parent_word_list, key = lambda x: len(x), reverse=True)
```

After running the LDA and NMF models with different numbers of pre-specified topics to be discovered, we can see that  

```{python topic_modelling_setup}
# Modules
from sklearn.pipeline import Pipeline
from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer
from sklearn.decomposition import LatentDirichletAllocation, NMF

# tokenizer
def tokenize_zh(doc):
  return jieba.cut(doc)

# Preprocessor
def preprocessor_zh(doc):
  regex_punctuation = r"[\d+\s+\n\t]|[\s+\.\!\/_,$%^*(+\"\']+|[+——！，。？、~@#￥%……&*（）]+|[【】╮╯▽╰╭★→「」]+|[！，❤。～《》：（）【】「」？”“；：、]"
  return re.sub(regex_punctuation, "", doc)

# Extracting the news article texts
news_text = all_news_articles.Text
```

```{python lda_perplexity_test}
lda_count = CountVectorizer(preprocessor=preprocessor_zh, 
                                            tokenizer=tokenize_zh, 
                                            stop_words=stop_words_full, 
                                            min_df=0.02)
lda_count_matrix = lda_count.fit_transform(news_text)
lda_perplexity = {}

for i in range(1, 21):
  lda_model = LatentDirichletAllocation(n_components=i, random_state=1, max_iter=100)
  _ = lda_model.fit(lda_count_matrix)
  lda_perplexity[f"Topic(s): {i}"] = lda_model.perplexity(lda_count_matrix)
```

```{python perplexity_plot}
sns.lineplot(x=lda_perplexity.keys(), y=lda_perplexity.values())
sns.scatterplot(x=lda_perplexity.keys(), y=lda_perplexity.values())
plt.gca().set(title="Perplexity score of LDA by number of topics", xlabel='Number of topics', ylabel='perplexity score', xticklabels=np.arange(1,21))
plt.tight_layout()
plt.show()
plt.clf()
```

```{python nmf_beta_divergence_test}
nmf_tfidf = TfidfVectorizer(preprocessor=preprocessor_zh, 
                            tokenizer=tokenize_zh, 
                            stop_words=stop_words_full, 
                            min_df=0.02)
nmf_term_matrix = nmf_tfidf.fit_transform(news_text)
nmf_beta_divergence = {}

for i in range(1, 21):
  nmf_model = NMF(n_components=i, max_iter=1000)
  _ = nmf_model.fit(nmf_term_matrix)
  nmf_beta_divergence[f"Topics(s): {i}"] = nmf_model.reconstruction_err_
```

```{python nmf_beta_plot}
sns.lineplot(x=nmf_beta_divergence.keys(), y=nmf_beta_divergence.values())
sns.scatterplot(x=nmf_beta_divergence.keys(), y=nmf_beta_divergence.values())
plt.gca().set(title='Beta divergence of NMF model by number of topics', xlabel='Number of topics', ylabel='Beta divergence score', xticklabels=np.arange(1,21))
plt.tight_layout()
plt.show()
plt.clf()
```

```{python function_extract_topic_words}
def topic_words(model, vectorizer, top_n_words):
  vocabulary = vectorizer.get_feature_names()
  for idx, topic in enumerate(model.components_):
    print(f"\nTopic {idx + 1}: ")
    print(" ".join([vocabulary[i] for i in topic.argsort()[:-top_n_words - 1: -1]]))
```

```{python lda_topics_words}
lda_topics = LatentDirichletAllocation(n_components=10, random_state=1, max_iter=100)
_ = lda_topics.fit(lda_count_matrix)
topic_words(lda_topics, lda_count, 20)
```

```{python nmf_topic_words}
nmf_topics = NMF(n_components=10, max_iter=1000)
_ = nmf_topics.fit(nmf_term_matrix)
topic_words(nmf_topics, nmf_tfidf, 20)
```


