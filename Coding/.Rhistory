replace_regex = r"^Total.*|^={2,}|^DOCUMENT ID:.*|^Source:.*|^Wisers.*|^Copyright\s\(c\).*|https?://.+|^Text\sSnapshot:.*|.+@.+\.[a-z]+"
with open(path, "r", encoding='utf-8') as file:
text = [line.strip('\n').strip() for line in file.readlines()]
text = [*map(lambda line: re.sub(replace_regex, '', line), text)]
text = [*filter(None, text)]
text = ''.join([' split ' if re.match(
r'^-+', line) else line for line in text])
text = re.split(r'\ssplit\s', text)
metadata_regex = r"\d+\.\s+(.*)\s+-\s+\((.*)\)\s+(\d{4}-\d{2}-\d{2})\s+(.*)"
metadata_list = []
main_text_list = []
for i in range(0, len(text)-1, 2):
metadata_list.extend(re.findall(metadata_regex, text[i]))
main_text_list.append(text[i+1])
return [*map(lambda x: str(x).strip('()'), metadata_list)], main_text_list
# Making the parsed text into a dataframe
def make_df(metadata, news_text):
news_df = pd.DataFrame()
news_df[['newspaper', 'headline', 'date', 'category_data']] = pd.Series(metadata)\
.str.split(', ', expand=True)\
.apply(lambda x: x.str.strip("'"))
news_df['text'] = news_text
news_df['date'] = pd.to_datetime(news_df.date)
return news_df
# Creating the dataframe
news_articles_all = pd.DataFrame()
news_text = glob.glob(r"news_articles/*.txt")
for file in news_text:
parsed_file_metadata, parsed_file_text = parse_text(file)
file_df = make_df(parsed_file_metadata, parsed_file_text)
news_articles_all = news_articles_all.append(file_df)
news_articles_all.drop_duplicates(subset=['newspaper', 'headline'], inplace=True)
# news_articles_all.to_csv('all_news.csv')
# Cleaning the data
news_articles_all['headline'] = news_articles_all.headline.apply(lambda x: ','.join(x))
View(news_text)
View(news_articles_all)
import glob
import pandas as pd
import numpy as np
import re
import unicodedata
# Parsing the text files of news with regex
def parse_text(path):
replace_regex = r"^Total.*|^={2,}|^DOCUMENT ID:.*|^Source:.*|^Wisers.*|^Copyright\s\(c\).*|https?://.+|^Text\sSnapshot:.*|.+@.+\.[a-z]+"
with open(path, "r", encoding='utf-8') as file:
text = [line.strip('\n').strip() for line in file.readlines()]
text = [*map(lambda line: re.sub(replace_regex, '', line), text)]
text = [*filter(None, text)]
text = ''.join([' split ' if re.match(
r'^-+', line) else line for line in text])
text = re.split(r'\ssplit\s', text)
metadata_regex = r"\d+\.\s+(.*)\s+-\s+\((.*)\)\s+(\d{4}-\d{2}-\d{2})\s+(.*)"
metadata_list = []
main_text_list = []
for i in range(0, len(text)-1, 2):
metadata_list.extend(re.findall(metadata_regex, text[i]))
main_text_list.append(text[i+1])
return [*map(lambda x: str(x).strip('()'), metadata_list)], main_text_list
# Making the parsed text into a dataframe
def make_df(metadata, news_text):
news_df = pd.DataFrame()
news_df[['newspaper', 'headline', 'date', 'category_data']] = pd.Series(metadata)\
.str.split(', ', expand=True)\
.apply(lambda x: x.str.strip("'"))
news_df['text'] = news_text
news_df['date'] = pd.to_datetime(news_df.date)
return news_df
# Creating the dataframe
news_articles_all = pd.DataFrame()
news_text = glob.glob(r"news_articles/*.txt")
for file in news_text:
parsed_file_metadata, parsed_file_text = parse_text(file)
file_df = make_df(parsed_file_metadata, parsed_file_text)
news_articles_all = news_articles_all.append(file_df)
news_articles_all.drop_duplicates(subset=['newspaper', 'headline'], inplace=True)
# news_articles_all.to_csv('all_news.csv')
# Cleaning the data
news_articles_all['headline'] = news_articles_all.headline.apply(lambda x: ''.join(x))
View(news_articles_all)
import glob
import pandas as pd
import numpy as np
import re
import unicodedata
# Parsing the text files of news with regex
def parse_text(path):
replace_regex = r"^Total.*|^={2,}|^DOCUMENT ID:.*|^Source:.*|^Wisers.*|^Copyright\s\(c\).*|https?://.+|^Text\sSnapshot:.*|.+@.+\.[a-z]+"
with open(path, "r", encoding='utf-8') as file:
text = [line.strip('\n').strip() for line in file.readlines()]
text = [*map(lambda line: re.sub(replace_regex, '', line), text)]
text = [*filter(None, text)]
text = ''.join([' split ' if re.match(
r'^-+', line) else line for line in text])
text = re.split(r'\ssplit\s', text)
metadata_regex = r"\d+\.\s+(.*)\s+-\s+\((.*)\)\s+(\d{4}-\d{2}-\d{2})\s+(.*)"
metadata_list = []
main_text_list = []
for i in range(0, len(text)-1, 2):
metadata_list.extend(re.findall(metadata_regex, text[i]))
main_text_list.append(text[i+1])
return [*map(lambda x: str(x).strip('()'), metadata_list)], main_text_list
# Making the parsed text into a dataframe
def make_df(metadata, news_text):
news_df = pd.DataFrame()
news_df[['newspaper', 'headline', 'date', 'category_data']] = pd.Series(metadata)\
.str.split(', ', expand=True)\
.apply(lambda x: x.str.strip("'"))
news_df['text'] = news_text
news_df['date'] = pd.to_datetime(news_df.date)
return news_df
# Creating the dataframe
news_articles_all = pd.DataFrame()
news_text = glob.glob(r"news_articles/*.txt")
for file in news_text:
parsed_file_metadata, parsed_file_text = parse_text(file)
file_df = make_df(parsed_file_metadata, parsed_file_text)
news_articles_all = news_articles_all.append(file_df)
news_articles_all.drop_duplicates(subset=['newspaper', 'headline'], inplace=True)
# news_articles_all.to_csv('all_news.csv')
# Cleaning the data
news_articles_all['headline'] = news_articles_all.headline.apply(lambda x: ''.join(x)).str.replace("\u3000", "")
import glob
import pandas as pd
import numpy as np
import re
import unicodedata
# Parsing the text files of news with regex
def parse_text(path):
replace_regex = r"^Total.*|^={2,}|^DOCUMENT ID:.*|^Source:.*|^Wisers.*|^Copyright\s\(c\).*|https?://.+|^Text\sSnapshot:.*|.+@.+\.[a-z]+"
with open(path, "r", encoding='utf-8') as file:
text = [line.strip('\n').strip() for line in file.readlines()]
text = [*map(lambda line: re.sub(replace_regex, '', line), text)]
text = [*filter(None, text)]
text = ''.join([' split ' if re.match(
r'^-+', line) else line for line in text])
text = re.split(r'\ssplit\s', text)
metadata_regex = r"\d+\.\s+(.*)\s+-\s+\((.*)\)\s+(\d{4}-\d{2}-\d{2})\s+(.*)"
metadata_list = []
main_text_list = []
for i in range(0, len(text)-1, 2):
metadata_list.extend(re.findall(metadata_regex, text[i]))
main_text_list.append(text[i+1])
return [*map(lambda x: str(x).strip('()'), metadata_list)], main_text_list
# Making the parsed text into a dataframe
def make_df(metadata, news_text):
news_df = pd.DataFrame()
news_df[['newspaper', 'headline', 'date', 'category_data']] = pd.Series(metadata)\
.str.split(', ', expand=True)\
.apply(lambda x: x.str.strip("'"))
news_df['text'] = news_text
news_df['date'] = pd.to_datetime(news_df.date)
return news_df
# Creating the dataframe
news_articles_all = pd.DataFrame()
news_text = glob.glob(r"news_articles/*.txt")
for file in news_text:
parsed_file_metadata, parsed_file_text = parse_text(file)
file_df = make_df(parsed_file_metadata, parsed_file_text)
news_articles_all = news_articles_all.append(file_df)
news_articles_all.drop_duplicates(subset=['newspaper', 'headline'], inplace=True)
# news_articles_all.to_csv('all_news.csv')
# Cleaning the data
news_articles_all['headline'] = news_articles_all.headline.apply(lambda x: ''.join(str(x))).str.replace("\u3000", "")
import glob
import pandas as pd
import numpy as np
import re
import unicodedata
# Parsing the text files of news with regex
def parse_text(path):
replace_regex = r"^Total.*|^={2,}|^DOCUMENT ID:.*|^Source:.*|^Wisers.*|^Copyright\s\(c\).*|https?://.+|^Text\sSnapshot:.*|.+@.+\.[a-z]+"
with open(path, "r", encoding='utf-8') as file:
text = [line.strip('\n').strip() for line in file.readlines()]
text = [*map(lambda line: re.sub(replace_regex, '', line), text)]
text = [*filter(None, text)]
text = ''.join([' split ' if re.match(
r'^-+', line) else line for line in text])
text = re.split(r'\ssplit\s', text)
metadata_regex = r"\d+\.\s+(.*)\s+-\s+\((.*)\)\s+(\d{4}-\d{2}-\d{2})\s+(.*)"
metadata_list = []
main_text_list = []
for i in range(0, len(text)-1, 2):
metadata_list.extend(re.findall(metadata_regex, text[i]))
main_text_list.append(text[i+1])
return [*map(lambda x: str(x).strip('()'), metadata_list)], main_text_list
# Making the parsed text into a dataframe
def make_df(metadata, news_text):
news_df = pd.DataFrame()
news_df[['newspaper', 'headline', 'date', 'category_data']] = pd.Series(metadata)\
.str.split(', ', expand=True)\
.apply(lambda x: x.str.strip("'"))
news_df['text'] = news_text
news_df['date'] = pd.to_datetime(news_df.date)
return news_df
# Creating the dataframe
news_articles_all = pd.DataFrame()
news_text = glob.glob(r"news_articles/*.txt")
for file in news_text:
parsed_file_metadata, parsed_file_text = parse_text(file)
file_df = make_df(parsed_file_metadata, parsed_file_text)
news_articles_all = news_articles_all.append(file_df)
news_articles_all.drop_duplicates(subset=['newspaper', 'headline'], inplace=True)
# news_articles_all.to_csv('all_news.csv')
# Cleaning the data
news_articles_all['headline'] = news_articles_all.headline.apply(lambda x: ', '.join(str(x))).str.replace("\u3000", "")
View(news_articles_all)
import glob
import pandas as pd
import numpy as np
import re
import unicodedata
# Parsing the text files of news with regex
def parse_text(path):
replace_regex = r"^Total.*|^={2,}|^DOCUMENT ID:.*|^Source:.*|^Wisers.*|^Copyright\s\(c\).*|https?://.+|^Text\sSnapshot:.*|.+@.+\.[a-z]+"
with open(path, "r", encoding='utf-8') as file:
text = [line.strip('\n').strip() for line in file.readlines()]
text = [*map(lambda line: re.sub(replace_regex, '', line), text)]
text = [*filter(None, text)]
text = ''.join([' split ' if re.match(
r'^-+', line) else line for line in text])
text = re.split(r'\ssplit\s', text)
metadata_regex = r"\d+\.\s+(.*)\s+-\s+\((.*)\)\s+(\d{4}-\d{2}-\d{2})\s+(.*)"
metadata_list = []
main_text_list = []
for i in range(0, len(text)-1, 2):
metadata_list.extend(re.findall(metadata_regex, text[i]))
main_text_list.append(text[i+1])
return [*map(lambda x: str(x).strip('()'), metadata_list)], main_text_list
# Making the parsed text into a dataframe
def make_df(metadata, news_text):
news_df = pd.DataFrame()
news_df[['newspaper', 'headline', 'date', 'category_data']] = pd.Series(metadata)\
.str.split(', ', expand=True)\
.apply(lambda x: x.str.strip("'"))
news_df['text'] = news_text
news_df['date'] = pd.to_datetime(news_df.date)
return news_df
# Creating the dataframe
news_articles_all = pd.DataFrame()
news_text = glob.glob(r"news_articles/*.txt")
for file in news_text:
parsed_file_metadata, parsed_file_text = parse_text(file)
file_df = make_df(parsed_file_metadata, parsed_file_text)
news_articles_all = news_articles_all.append(file_df)
news_articles_all.drop_duplicates(subset=['newspaper', 'headline'], inplace=True)
# news_articles_all.to_csv('all_news.csv')
# Cleaning the data
news_articles_all['headline'] = news_articles_all.headline.apply(lambda x: ''.join(str(x))).str.replace("\u3000", "")
View(news_articles_all)
View(news_articles_all)
import glob
import pandas as pd
import numpy as np
import re
import unicodedata
# Parsing the text files of news with regex
def parse_text(path):
replace_regex = r"^Total.*|^={2,}|^DOCUMENT ID:.*|^Source:.*|^Wisers.*|^Copyright\s\(c\).*|https?://.+|^Text\sSnapshot:.*|.+@.+\.[a-z]+"
with open(path, "r", encoding='utf-8') as file:
text = [line.strip('\n').strip() for line in file.readlines()]
text = [*map(lambda line: re.sub(replace_regex, '', line), text)]
text = [*filter(None, text)]
text = ''.join([' split ' if re.match(
r'^-+', line) else line for line in text])
text = re.split(r'\ssplit\s', text)
metadata_regex = r"\d+\.\s+(.*)\s+-\s+\((.*)\)\s+(\d{4}-\d{2}-\d{2})\s+(.*)"
metadata_list = []
main_text_list = []
for i in range(0, len(text)-1, 2):
metadata_list.extend(re.findall(metadata_regex, text[i]))
main_text_list.append(text[i+1])
return [*map(lambda x: str(x).strip('()'), metadata_list)], main_text_list
# Making the parsed text into a dataframe
def make_df(metadata, news_text):
news_df = pd.DataFrame()
news_df[['newspaper', 'headline', 'date', 'category_data']] = pd.Series(metadata)\
.str.split(', ', expand=True)\
.apply(lambda x: x.str.strip("'"))
news_df['text'] = news_text
news_df['date'] = pd.to_datetime(news_df.date)
return news_df
# Creating the dataframe
news_articles_all = pd.DataFrame()
news_text = glob.glob(r"news_articles/*.txt")
for file in news_text:
parsed_file_metadata, parsed_file_text = parse_text(file)
file_df = make_df(parsed_file_metadata, parsed_file_text)
news_articles_all = news_articles_all.append(file_df)
news_articles_all.drop_duplicates(subset=['newspaper', 'headline'], inplace=True)
# news_articles_all.to_csv('all_news.csv')
# Cleaning the data
news_articles_all['headline'] = news_articles_all.headline.apply(lambda x: ','.join(str(x))).str.replace(r"[\u3000]", "")
View(news_articles_all)
View(news_articles_all)
news_articles_all['headline'] = news_articles_all.headline.apply(lambda x: ','.join(x))
View(news_articles_all)
import glob
import pandas as pd
import numpy as np
import re
import unicodedata
# Parsing the text files of news with regex
def parse_text(path):
replace_regex = r"^Total.*|^={2,}|^DOCUMENT ID:.*|^Source:.*|^Wisers.*|^Copyright\s\(c\).*|https?://.+|^Text\sSnapshot:.*|.+@.+\.[a-z]+"
with open(path, "r", encoding='utf-8') as file:
text = [line.strip('\n').strip() for line in file.readlines()]
text = [*map(lambda line: re.sub(replace_regex, '', line), text)]
text = [*filter(None, text)]
text = ''.join([' split ' if re.match(
r'^-+', line) else line for line in text])
text = re.split(r'\ssplit\s', text)
metadata_regex = r"\d+\.\s+(.*)\s+-\s+\((.*)\)\s+(\d{4}-\d{2}-\d{2})\s+(.*)"
metadata_list = []
main_text_list = []
for i in range(0, len(text)-1, 2):
metadata_list.extend(re.findall(metadata_regex, text[i]))
main_text_list.append(text[i+1])
return [*map(lambda x: str(x).strip('()'), metadata_list)], main_text_list
# Making the parsed text into a dataframe
def make_df(metadata, news_text):
news_df = pd.DataFrame()
news_df[['newspaper', 'headline', 'date', 'category_data']] = pd.Series(metadata)\
.str.split(', ', expand=True)\
.apply(lambda x: x.str.strip("'"))
news_df['text'] = news_text
news_df['date'] = pd.to_datetime(news_df.date)
return news_df
# Creating the dataframe
news_articles_all = pd.DataFrame()
news_text = glob.glob(r"news_articles/*.txt")
for file in news_text:
parsed_file_metadata, parsed_file_text = parse_text(file)
file_df = make_df(parsed_file_metadata, parsed_file_text)
news_articles_all = news_articles_all.append(file_df)
news_articles_all.drop_duplicates(subset=['newspaper', 'headline'], inplace=True)
# news_articles_all.to_csv('all_news.csv')
# Cleaning the data
news_articles_all['headline'] = news_articles_all.headline.apply(lambda x: ','.join(x))
View(news_articles_all)
import glob
import pandas as pd
import numpy as np
import re
import unicodedata
# Parsing the text files of news with regex
def parse_text(path):
replace_regex = r"^Total.*|^={2,}|^DOCUMENT ID:.*|^Source:.*|^Wisers.*|^Copyright\s\(c\).*|https?://.+|^Text\sSnapshot:.*|.+@.+\.[a-z]+"
with open(path, "r", encoding='utf-8') as file:
text = [line.strip('\n').strip() for line in file.readlines()]
text = [*map(lambda line: re.sub(replace_regex, '', line), text)]
text = [*filter(None, text)]
text = ''.join([' split ' if re.match(
r'^-+', line) else line for line in text])
text = re.split(r'\ssplit\s', text)
metadata_regex = r"\d+\.\s+(.*)\s+-\s+\((.*)\)\s+(\d{4}-\d{2}-\d{2})\s+(.*)"
metadata_list = []
main_text_list = []
for i in range(0, len(text)-1, 2):
metadata_list.extend(re.findall(metadata_regex, text[i]))
main_text_list.append(text[i+1])
return [*map(lambda x: str(x).strip('()'), metadata_list)], main_text_list
# Making the parsed text into a dataframe
def make_df(metadata, news_text):
news_df = pd.DataFrame()
news_df[['newspaper', 'headline', 'date', 'category_data']] = pd.Series(metadata)\
.str.split(', ', expand=True)\
.apply(lambda x: x.str.strip("'"))
news_df['text'] = news_text
news_df['date'] = pd.to_datetime(news_df.date)
return news_df
# Creating the dataframe
news_articles_all = pd.DataFrame()
news_text = glob.glob(r"news_articles/*.txt")
for file in news_text:
parsed_file_metadata, parsed_file_text = parse_text(file)
file_df = make_df(parsed_file_metadata, parsed_file_text)
news_articles_all = news_articles_all.append(file_df)
news_articles_all.drop_duplicates(subset=['newspaper', 'headline'], inplace=True)
# news_articles_all.to_csv('all_news.csv')
# Cleaning the data
news_articles_all['headline'] = news_articles_all.headline.apply(lambda x: ','.join(x)).str.replace(r"[,\u30]", "")
import glob
import pandas as pd
import numpy as np
import re
import unicodedata
# Parsing the text files of news with regex
def parse_text(path):
replace_regex = r"^Total.*|^={2,}|^DOCUMENT ID:.*|^Source:.*|^Wisers.*|^Copyright\s\(c\).*|https?://.+|^Text\sSnapshot:.*|.+@.+\.[a-z]+"
with open(path, "r", encoding='utf-8') as file:
text = [line.strip('\n').strip() for line in file.readlines()]
text = [*map(lambda line: re.sub(replace_regex, '', line), text)]
text = [*filter(None, text)]
text = ''.join([' split ' if re.match(
r'^-+', line) else line for line in text])
text = re.split(r'\ssplit\s', text)
metadata_regex = r"\d+\.\s+(.*)\s+-\s+\((.*)\)\s+(\d{4}-\d{2}-\d{2})\s+(.*)"
metadata_list = []
main_text_list = []
for i in range(0, len(text)-1, 2):
metadata_list.extend(re.findall(metadata_regex, text[i]))
main_text_list.append(text[i+1])
return [*map(lambda x: str(x).strip('()'), metadata_list)], main_text_list
# Making the parsed text into a dataframe
def make_df(metadata, news_text):
news_df = pd.DataFrame()
news_df[['newspaper', 'headline', 'date', 'category_data']] = pd.Series(metadata)\
.str.split(', ', expand=True)\
.apply(lambda x: x.str.strip("'"))
news_df['text'] = news_text
news_df['date'] = pd.to_datetime(news_df.date)
return news_df
# Creating the dataframe
news_articles_all = pd.DataFrame()
news_text = glob.glob(r"news_articles/*.txt")
for file in news_text:
parsed_file_metadata, parsed_file_text = parse_text(file)
file_df = make_df(parsed_file_metadata, parsed_file_text)
news_articles_all = news_articles_all.append(file_df)
news_articles_all.drop_duplicates(subset=['newspaper', 'headline'], inplace=True)
# news_articles_all.to_csv('all_news.csv')
# Cleaning the data
news_articles_all['headline'] = news_articles_all.headline.apply(lambda x: ','.join(x)).str.replace(r"[,\\u30]", "")
View(news_articles_all)
import glob
import pandas as pd
import numpy as np
import re
import unicodedata
# Parsing the text files of news with regex
def parse_text(path):
replace_regex = r"^Total.*|^={2,}|^DOCUMENT ID:.*|^Source:.*|^Wisers.*|^Copyright\s\(c\).*|https?://.+|^Text\sSnapshot:.*|.+@.+\.[a-z]+"
with open(path, "r", encoding='utf-8') as file:
text = [line.strip('\n').strip() for line in file.readlines()]
text = [*map(lambda line: re.sub(replace_regex, '', line), text)]
text = [*filter(None, text)]
text = ''.join([' split ' if re.match(
r'^-+', line) else line for line in text])
text = re.split(r'\ssplit\s', text)
metadata_regex = r"\d+\.\s+(.*)\s+-\s+\((.*)\)\s+(\d{4}-\d{2}-\d{2})\s+(.*)"
metadata_list = []
main_text_list = []
for i in range(0, len(text)-1, 2):
metadata_list.extend(re.findall(metadata_regex, text[i]))
main_text_list.append(text[i+1])
return [*map(lambda x: str(x).strip('()'), metadata_list)], main_text_list
# Making the parsed text into a dataframe
def make_df(metadata, news_text):
news_df = pd.DataFrame()
news_df[['newspaper', 'headline', 'date', 'category_data']] = pd.Series(metadata)\
.str.split(', ', expand=True)\
.apply(lambda x: x.str.strip("'"))
news_df['text'] = news_text
news_df['date'] = pd.to_datetime(news_df.date)
return news_df
# Creating the dataframe
news_articles_all = pd.DataFrame()
news_text = glob.glob(r"news_articles/*.txt")
for file in news_text:
parsed_file_metadata, parsed_file_text = parse_text(file)
file_df = make_df(parsed_file_metadata, parsed_file_text)
news_articles_all = news_articles_all.append(file_df)
news_articles_all.drop_duplicates(subset=['newspaper', 'headline'], inplace=True)
# news_articles_all.to_csv('all_news.csv')
# Cleaning the data
news_articles_all['headline'] = news_articles_all.headline\
.apply(lambda x: ','.join(x))/
.str.replace(r"[,\\u30]", "")
import glob
import pandas as pd
import numpy as np
import re
import unicodedata
# Parsing the text files of news with regex
def parse_text(path):
replace_regex = r"^Total.*|^={2,}|^DOCUMENT ID:.*|^Source:.*|^Wisers.*|^Copyright\s\(c\).*|https?://.+|^Text\sSnapshot:.*|.+@.+\.[a-z]+"
with open(path, "r", encoding='utf-8') as file:
text = [line.strip('\n').strip() for line in file.readlines()]
text = [*map(lambda line: re.sub(replace_regex, '', line), text)]
text = [*filter(None, text)]
text = ''.join([' split ' if re.match(
r'^-+', line) else line for line in text])
text = re.split(r'\ssplit\s', text)
metadata_regex = r"\d+\.\s+(.*)\s+-\s+\((.*)\)\s+(\d{4}-\d{2}-\d{2})\s+(.*)"
metadata_list = []
main_text_list = []
for i in range(0, len(text)-1, 2):
metadata_list.extend(re.findall(metadata_regex, text[i]))
main_text_list.append(text[i+1])
return [*map(lambda x: str(x).strip('()'), metadata_list)], main_text_list
# Making the parsed text into a dataframe
def make_df(metadata, news_text):
news_df = pd.DataFrame()
news_df[['newspaper', 'headline', 'date', 'category_data']] = pd.Series(metadata)\
.str.split(', ', expand=True)\
.apply(lambda x: x.str.strip("'"))
news_df['text'] = news_text
news_df['date'] = pd.to_datetime(news_df.date)
return news_df
# Creating the dataframe
news_articles_all = pd.DataFrame()
news_text = glob.glob(r"news_articles/*.txt")
for file in news_text:
parsed_file_metadata, parsed_file_text = parse_text(file)
file_df = make_df(parsed_file_metadata, parsed_file_text)
news_articles_all = news_articles_all.append(file_df)
news_articles_all.drop_duplicates(subset=['newspaper', 'headline'], inplace=True)
# news_articles_all.to_csv('all_news.csv')
# Cleaning the data
news_articles_all['headline'] = news_articles_all.headline.apply(lambda x: ','.join(x)) /
.str.replace(r"[,\\u30]", "")
