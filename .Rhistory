import seaborn as sns
import jieba
import zhon.hanzi
import re
from itertools import chain
pd.set_option('display.max_columns', None)
sns.set_theme(context='paper')
warnings.filterwarnings(category=FutureWarning, action = 'ignore')
news_df = pd.read_csv("Coding/asylum_seekers_articles_final.csv")
num_of_articles = news_df.shape[0]
num_of_newspapers = news_df.Newspaper.nunique()
fig, axes = plt.subplots(1,2)
# Plotting by-outlet amount of articles
articles_outlet = news_df.value_counts('Newspaper')
sns.barplot(x=articles_outlet, y=articles_outlet.index, ax=axes[0])
axes[0].set(xlabel='Number of articles (in base-10 log scale)', ylabel='', title='By newspaper', xscale="log")
for idx, value in enumerate(articles_outlet):
axes[0].text(value + 5, idx + 0.2, value)
# Plotting by-camp amount of articles
sns.countplot(x='Political_camp', data=news_df, ax=axes[1])
axes[1].set(xlabel='', ylabel='Number of articles', title='By political camp')
count_by_political_camp = news_df['Political_camp'].value_counts(sort=False)
for idx, value in enumerate(count_by_political_camp):
axes[1].text(idx - 0.1, value + 5, value)
axes[1].set_xticklabels(['Pro-Beijing', 'Neutral', 'Pro-democracy'], rotation=60)
plt.tight_layout()
plt.show()
plt.clf()
articles_by_month = news_df.Month.value_counts(sort=False)
ax = sns.lineplot(x=articles_by_month.index, y=articles_by_month, color='tab:orange')
ax.set(xlabel='Month', ylabel='Number of articles', xticks=np.arange(1,13))
plt.tight_layout()
plt.show()
plt.clf()
sentiment_camp = pd.crosstab(news_df.Political_camp, news_df.Sentiment, margins=True)
sentiment_camp.columns = ["Negative", "Neutral", "Positive", "All"]
quit
knitr::kable(py$sentiment_camp, digits = 4, caption="Polarities of the news articles on asylum seekers in Hong Kong in 2019")
reticulate::repl_python()
ax = sns.countplot(x="Racial_label", hue="Sentiment", data=news_df)
ax.set(xlabel="Presence of racial labels", ylabel="Number of articles")
ax.set_xticklabels(["No", "Yes"])
plt.show()
plt.clf()
fig, axes = plt.subplots(1, 2)
# Plotting distribution of title word count
sns.histplot(x='Title_length', data=news_df, ax=axes[0], color='tab:blue', alpha=0.5)
axes[0].set(xlabel='Word count', title='Article title')
mean_title_length = news_df.Title_length.mean()
axes[0].axvline(mean_title_length, alpha=0.5, linestyle = '-.', c='black', label='Mean of title length')
axes[0].legend()
# Plotting distribution of article word count
sns.histplot(x='Raw_article_length', data=news_df, ax=axes[1], color='tab:orange', alpha=0.5)
axes[1].set(xlabel='Word count (in thousands)', title='Raw article text')
mean_article_length = news_df.Raw_article_length.mean()
axes[1].axvline(mean_article_length, alpha=0.5, linestyle = '--', c='black', label='Mean of main text length')
axes[1].legend()
# Global setup
plt.tight_layout()
plt.show()
plt.clf()
article_length_summary = news_df[['Title_length', 'Raw_article_length']].describe()
quit
knitr::kable(py$article_length_summary, col.names = c("Title", "Raw main text"), caption="Summary statistics of the word counts of the news articles' titles and main texts")
reticulate::repl_python()
news_df["Political_camp"] = pd.Categorical(news_df["Political_camp"], categories=['Pro-Beijing', 'Neutral', 'Pro-democracy'])  # So that pro-Beijing becomes the reference category
news_df = pd.get_dummies(news_df, columns=["Political_camp", "Month"], drop_first=True)
news_df["Article"] = news_df.Title.str.cat(news_df.Text, sep=" ")
news_df.drop(columns=["Title", "Text"], inplace=True)
def read_text(path):
with open(path, 'r', encoding='utf-8') as file:
text = file.readlines()
text = [word.replace('\n', '') for word in text]
return text
hk_politics_words = read_text('Coding/HKPolDict-master/merged.txt') # Hong Kong politics
asylum_seeker_words = read_text('Coding/Asylum_seeker_words.txt') # Asylum seekers
for word in chain(hk_politics_words, asylum_seeker_words):
jieba.add_word(word)
# Creating the stop word list
stop_words_cantonese = read_text('Coding/text_cleaning/cantonese_stopwords.txt')
punctuations = [punc for punc in zhon.hanzi.punctuation]
stop_words_full = list(set(word for word in chain(stop_words_cantonese, punctuations)))
# Modules
from sklearn.pipeline import Pipeline
from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer
from sklearn.decomposition import LatentDirichletAllocation, NMF
# tokenizer
def tokenize_zh(doc):
return jieba.cut(doc)
# Preprocessor
def preprocessor_zh(doc):
regex_punctuation = r"[\d+\s+\n\t]|[\s+\.\!\/_,$%^*(+\"\']+|[+——！，。？、~@#￥%……&*（）]+|[【】╮╯▽╰╭★→「」]+|[！，❤。～《》：（）【】「」？”“；：、]"
return re.sub(regex_punctuation, "", doc)
# Extracting the news article texts
news_articles = news_df.Article
quit
library(reticulate)
reticulate::repl_python()
import warnings
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import jieba
import zhon.hanzi
import re
from itertools import chain
pd.set_option('display.max_columns', None)
sns.set_theme(context='paper')
warnings.filterwarnings(category=FutureWarning, action = 'ignore')
news_df = pd.read_csv("Coding/asylum_seekers_articles_final.csv")
num_of_articles = news_df.shape[0]
num_of_newspapers = news_df.Newspaper.nunique()
fig, axes = plt.subplots(1,2)
# Plotting by-outlet amount of articles
articles_outlet = news_df.value_counts('Newspaper')
sns.barplot(x=articles_outlet, y=articles_outlet.index, ax=axes[0])
axes[0].set(xlabel='Number of articles (in base-10 log scale)', ylabel='', title='By newspaper', xscale="log")
for idx, value in enumerate(articles_outlet):
axes[0].text(value + 5, idx + 0.2, value)
# Plotting by-camp amount of articles
sns.countplot(x='Political_camp', data=news_df, ax=axes[1])
axes[1].set(xlabel='', ylabel='Number of articles', title='By political camp')
count_by_political_camp = news_df['Political_camp'].value_counts(sort=False)
for idx, value in enumerate(count_by_political_camp):
axes[1].text(idx - 0.1, value + 5, value)
axes[1].set_xticklabels(['Pro-Beijing', 'Neutral', 'Pro-democracy'], rotation=60)
plt.tight_layout()
plt.show()
plt.clf()
articles_by_month = news_df.Month.value_counts(sort=False)
ax = sns.lineplot(x=articles_by_month.index, y=articles_by_month, color='tab:orange')
ax.set(xlabel='Month', ylabel='Number of articles', xticks=np.arange(1,13))
plt.tight_layout()
plt.show()
plt.clf()
sentiment_camp = pd.crosstab(news_df.Political_camp, news_df.Sentiment, margins=True)
sentiment_camp.columns = ["Negative", "Neutral", "Positive", "All"]
quit
knitr::kable(py$sentiment_camp, digits = 4, caption="Polarities of the news articles on asylum seekers in Hong Kong in 2019")
reticulate::repl_python()
ax = sns.countplot(x="Racial_label", hue="Sentiment", data=news_df)
ax.set(xlabel="Presence of racial labels", ylabel="Number of articles")
ax.set_xticklabels(["No", "Yes"])
plt.show()
plt.clf()
fig, axes = plt.subplots(1, 2)
# Plotting distribution of title word count
sns.histplot(x='Title_length', data=news_df, ax=axes[0], color='tab:blue', alpha=0.5)
axes[0].set(xlabel='Word count', title='Article title')
mean_title_length = news_df.Title_length.mean()
axes[0].axvline(mean_title_length, alpha=0.5, linestyle = '-.', c='black', label='Mean of title length')
axes[0].legend()
# Plotting distribution of article word count
sns.histplot(x='Raw_article_length', data=news_df, ax=axes[1], color='tab:orange', alpha=0.5)
axes[1].set(xlabel='Word count (in thousands)', title='Raw article text')
mean_article_length = news_df.Raw_article_length.mean()
axes[1].axvline(mean_article_length, alpha=0.5, linestyle = '--', c='black', label='Mean of main text length')
axes[1].legend()
# Global setup
plt.tight_layout()
plt.show()
plt.clf()
article_length_summary = news_df[['Title_length', 'Raw_article_length']].describe()
quit
knitr::kable(py$article_length_summary, col.names = c("Title", "Raw main text"), caption="Summary statistics of the word counts of the news articles' titles and main texts")
reticulate::repl_python()
news_df["Political_camp"] = pd.Categorical(news_df["Political_camp"], categories=['Pro-Beijing', 'Neutral', 'Pro-democracy'])  # So that pro-Beijing becomes the reference category
news_df = pd.get_dummies(news_df, columns=["Political_camp", "Month"], drop_first=True)
news_df["Article"] = news_df.Title.str.cat(news_df.Text, sep=" ")
news_df.drop(columns=["Title", "Text"], inplace=True)
def read_text(path):
with open(path, 'r', encoding='utf-8') as file:
text = file.readlines()
text = [word.replace('\n', '') for word in text]
return text
hk_politics_words = read_text('Coding/HKPolDict-master/merged.txt') # Hong Kong politics
asylum_seeker_words = read_text('Coding/Asylum_seeker_words.txt') # Asylum seekers
for word in chain(hk_politics_words, asylum_seeker_words):
jieba.add_word(word)
# Creating the stop word list
stop_words_cantonese = read_text('Coding/text_cleaning/cantonese_stopwords.txt')
punctuations = [punc for punc in zhon.hanzi.punctuation]
stop_words_full = list(set(word for word in chain(stop_words_cantonese, punctuations)))
# Modules
from sklearn.pipeline import Pipeline
from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer
from sklearn.decomposition import LatentDirichletAllocation, NMF
# tokenizer
def tokenize_zh(doc):
return jieba.cut(doc)
# Preprocessor
def preprocessor_zh(doc):
regex_punctuation = r"[\d+\s+\n\t]|[\s+\.\!\/_,$%^*(+\"\']+|[+——！，。？、~@#￥%……&*（）]+|[【】╮╯▽╰╭★→「」]+|[！，❤。～《》：（）【】「」？”“；：、]"
return re.sub(regex_punctuation, "", doc)
# Extracting the news article texts
news_articles = news_df.Article
lda_count = CountVectorizer(preprocessor=preprocessor_zh,
tokenizer=tokenize_zh,
stop_words=stop_words_full,
min_df=0.02)
lda_count_matrix = lda_count.fit_transform(news_text)
def topic_words(model, vectorizer, top_n_words):
vocabulary = vectorizer.get_feature_names()
for idx, topic in enumerate(model.components_):
print(f"\nTopic {idx + 1}: ")
print(" ".join([vocabulary[i] for i in topic.argsort()[:-top_n_words - 1: -1]]))
lda_topics = LatentDirichletAllocation(n_components=10, random_state=1, max_iter=100)
_ = lda_topics.fit(lda_count_matrix)
topic_words(lda_topics, lda_count, 20)
quit
library(reticulate)
reticulate::repl_python()
import warnings
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import jieba
import zhon.hanzi
import re
from itertools import chain
pd.set_option('display.max_columns', None)
sns.set_theme(context='paper')
warnings.filterwarnings(category=FutureWarning, action = 'ignore')
news_df = pd.read_csv("Coding/asylum_seekers_articles_final.csv")
num_of_articles = news_df.shape[0]
num_of_newspapers = news_df.Newspaper.nunique()
fig, axes = plt.subplots(1,2)
# Plotting by-outlet amount of articles
articles_outlet = news_df.value_counts('Newspaper')
sns.barplot(x=articles_outlet, y=articles_outlet.index, ax=axes[0])
axes[0].set(xlabel='Number of articles (in base-10 log scale)', ylabel='', title='By newspaper', xscale="log")
for idx, value in enumerate(articles_outlet):
axes[0].text(value + 5, idx + 0.2, value)
# Plotting by-camp amount of articles
sns.countplot(x='Political_camp', data=news_df, ax=axes[1])
axes[1].set(xlabel='', ylabel='Number of articles', title='By political camp')
count_by_political_camp = news_df['Political_camp'].value_counts(sort=False)
for idx, value in enumerate(count_by_political_camp):
axes[1].text(idx - 0.1, value + 5, value)
axes[1].set_xticklabels(['Pro-Beijing', 'Neutral', 'Pro-democracy'], rotation=60)
plt.tight_layout()
plt.show()
plt.clf()
articles_by_month = news_df.Month.value_counts(sort=False)
ax = sns.lineplot(x=articles_by_month.index, y=articles_by_month, color='tab:orange')
ax.set(xlabel='Month', ylabel='Number of articles', xticks=np.arange(1,13))
plt.tight_layout()
plt.show()
plt.clf()
sentiment_camp = pd.crosstab(news_df.Political_camp, news_df.Sentiment, margins=True)
sentiment_camp.columns = ["Negative", "Neutral", "Positive", "All"]
quit
knitr::kable(py$sentiment_camp, digits = 4, caption="Polarities of the news articles on asylum seekers in Hong Kong in 2019")
reticulate::repl_python()
ax = sns.countplot(x="Racial_label", hue="Sentiment", data=news_df)
ax.set(xlabel="Presence of racial labels", ylabel="Number of articles")
ax.set_xticklabels(["No", "Yes"])
plt.show()
plt.clf()
fig, axes = plt.subplots(1, 2)
# Plotting distribution of title word count
sns.histplot(x='Title_length', data=news_df, ax=axes[0], color='tab:blue', alpha=0.5)
axes[0].set(xlabel='Word count', title='Article title')
mean_title_length = news_df.Title_length.mean()
axes[0].axvline(mean_title_length, alpha=0.5, linestyle = '-.', c='black', label='Mean of title length')
axes[0].legend()
# Plotting distribution of article word count
sns.histplot(x='Raw_article_length', data=news_df, ax=axes[1], color='tab:orange', alpha=0.5)
axes[1].set(xlabel='Word count (in thousands)', title='Raw article text')
mean_article_length = news_df.Raw_article_length.mean()
axes[1].axvline(mean_article_length, alpha=0.5, linestyle = '--', c='black', label='Mean of main text length')
axes[1].legend()
# Global setup
plt.tight_layout()
plt.show()
plt.clf()
article_length_summary = news_df[['Title_length', 'Raw_article_length']].describe()
quit
knitr::kable(py$article_length_summary, col.names = c("Title", "Raw main text"), caption="Summary statistics of the word counts of the news articles' titles and main texts")
reticulate::repl_python()
news_df["Political_camp"] = pd.Categorical(news_df["Political_camp"], categories=['Pro-Beijing', 'Neutral', 'Pro-democracy'])  # So that pro-Beijing becomes the reference category
news_df = pd.get_dummies(news_df, columns=["Political_camp", "Month"], drop_first=True)
news_df["Article"] = news_df.Title.str.cat(news_df.Text, sep=" ")
news_df.drop(columns=["Title", "Text"], inplace=True)
def read_text(path):
with open(path, 'r', encoding='utf-8') as file:
text = file.readlines()
text = [word.replace('\n', '') for word in text]
return text
hk_politics_words = read_text('Coding/HKPolDict-master/merged.txt') # Hong Kong politics
asylum_seeker_words = read_text('Coding/Asylum_seeker_words.txt') # Asylum seekers
for word in chain(hk_politics_words, asylum_seeker_words):
jieba.add_word(word)
# Creating the stop word list
stop_words_cantonese = read_text('Coding/text_cleaning/cantonese_stopwords.txt')
punctuations = [punc for punc in zhon.hanzi.punctuation]
stop_words_full = list(set(word for word in chain(stop_words_cantonese, punctuations)))
# Modules
from sklearn.pipeline import Pipeline
from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer
from sklearn.decomposition import LatentDirichletAllocation, NMF
# tokenizer
def tokenize_zh(doc):
return jieba.cut(doc)
# Preprocessor
def preprocessor_zh(doc):
regex_punctuation = r"[\d+\s+\n\t]|[\s+\.\!\/_,$%^*(+\"\']+|[+——！，。？、~@#￥%……&*（）]+|[【】╮╯▽╰╭★→「」]+|[！，❤。～《》：（）【】「」？”“；：、]"
return re.sub(regex_punctuation, "", doc)
# Extracting the news article texts
news_articles = news_df.Article
lda_count = CountVectorizer(preprocessor=preprocessor_zh,
tokenizer=tokenize_zh,
stop_words=stop_words_full,
min_df=0.02)
lda_count_matrix = lda_count.fit_transform(news_articles)
def topic_words(model, vectorizer, top_n_words):
vocabulary = vectorizer.get_feature_names()
for idx, topic in enumerate(model.components_):
print(f"\nTopic {idx + 1}: ")
print(" ".join([vocabulary[i] for i in topic.argsort()[:-top_n_words - 1: -1]]))
lda_topics = LatentDirichletAllocation(n_components=10, random_state=1, max_iter=100)
_ = lda_topics.fit(lda_count_matrix)
topic_words(lda_topics, lda_count, 20)
# Modules
from sklearn.pipeline import Pipeline
from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer
from sklearn.decomposition import LatentDirichletAllocation, NMF
# tokenizer
def tokenize_zh(doc):
return jieba.cut(doc)
# Preprocessor
def preprocessor_zh(doc):
regex_punctuation = r"[\d+\s+\n\t]|[\s+\.\!\/_,$%^*(+\"\']+|[+——！，。？、~@#￥%……&*（）]+|[【】╮╯▽╰╭★→「」]+|[！，❤。～《》：（）【】「」？”“；：、]"
return re.sub(regex_punctuation, "", doc)
# Extracting the news article texts
news_articles = news_df.Article
import warnings
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import jieba
import zhon.hanzi
import re
from itertools import chain
jieba.enable_paddle()
pd.set_option('display.max_columns', None)
sns.set_theme(context='paper')
warnings.filterwarnings(category=FutureWarning, action = 'ignore')
# Modules
from sklearn.pipeline import Pipeline
from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer
from sklearn.decomposition import LatentDirichletAllocation, NMF
# tokenizer
def tokenize_zh(doc):
return jieba.cut(doc, cut_all=True)
# Preprocessor
def preprocessor_zh(doc):
regex_punctuation = r"[\d+\s+\n\t]|[\s+\.\!\/_,$%^*(+\"\']+|[+——！，。？、~@#￥%……&*（）]+|[【】╮╯▽╰╭★→「」]+|[！，❤。～《》：（）【】「」？”“；：、]"
return re.sub(regex_punctuation, "", doc)
# Extracting the news article texts
news_articles = news_df.Article
lda_count = CountVectorizer(preprocessor=preprocessor_zh,
tokenizer=tokenize_zh,
stop_words=stop_words_full,
min_df=0.02)
lda_count_matrix = lda_count.fit_transform(news_articles)
def topic_words(model, vectorizer, top_n_words):
vocabulary = vectorizer.get_feature_names()
for idx, topic in enumerate(model.components_):
print(f"\nTopic {idx + 1}: ")
print(" ".join([vocabulary[i] for i in topic.argsort()[:-top_n_words - 1: -1]]))
lda_topics = LatentDirichletAllocation(n_components=10, random_state=1, max_iter=100)
_ = lda_topics.fit(lda_count_matrix)
topic_words(lda_topics, lda_count, 20)
# Modules
from sklearn.pipeline import Pipeline
from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer
from sklearn.decomposition import LatentDirichletAllocation, NMF
# tokenizer
def tokenize_zh(doc):
return jieba.cut(doc)
# Preprocessor
def preprocessor_zh(doc):
regex_punctuation = r"[\d+\s+\n\t]|[\s+\.\!\/_,$%^*(+\"\']+|[+——！，。？、~@#￥%……&*（）]+|[【】╮╯▽╰╭★→「」]+|[！，❤。～《》：（）【】「」？”“；：、]"
return re.sub(regex_punctuation, "", doc)
# Extracting the news article texts
news_articles = news_df.Article
lda_count = CountVectorizer(preprocessor=preprocessor_zh,
tokenizer=tokenize_zh,
stop_words=stop_words_full,
min_df=0.02)
lda_count_matrix = lda_count.fit_transform(news_articles)
# tokenizer
def tokenize_zh(doc):
return jieba.cut(doc)
# Preprocessor
def preprocessor_zh(doc):
regex_punctuation = r"[\d+\s+\n\t]|[\s+\.\!\/_,$%^*(+\"\']+|[+——！，。？、~@#￥%……&*（）]+|[【】╮╯▽╰╭★→「」]+|[！，❤。～《》：（）【】「」？”“；：、]"
return re.sub(regex_punctuation, "", doc)
# Extracting the news article texts
news_articles = news_df.Article
quit
library(reticulate)
reticulate::repl_python()
import warnings
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import jieba
import zhon.hanzi
import re
from itertools import chain
pd.set_option('display.max_columns', None)
sns.set_theme(context='paper')
warnings.filterwarnings(category=FutureWarning, action = 'ignore')
news_df = pd.read_csv("Coding/asylum_seekers_articles_final.csv")
num_of_articles = news_df.shape[0]
num_of_newspapers = news_df.Newspaper.nunique()
fig, axes = plt.subplots(1,2)
# Plotting by-outlet amount of articles
articles_outlet = news_df.value_counts('Newspaper')
sns.barplot(x=articles_outlet, y=articles_outlet.index, ax=axes[0])
axes[0].set(xlabel='Number of articles (in base-10 log scale)', ylabel='', title='By newspaper', xscale="log")
for idx, value in enumerate(articles_outlet):
axes[0].text(value + 5, idx + 0.2, value)
# Plotting by-camp amount of articles
sns.countplot(x='Political_camp', data=news_df, ax=axes[1])
axes[1].set(xlabel='', ylabel='Number of articles', title='By political camp')
count_by_political_camp = news_df['Political_camp'].value_counts(sort=False)
for idx, value in enumerate(count_by_political_camp):
axes[1].text(idx - 0.1, value + 5, value)
axes[1].set_xticklabels(['Pro-Beijing', 'Neutral', 'Pro-democracy'], rotation=60)
plt.tight_layout()
plt.show()
plt.clf()
articles_by_month = news_df.Month.value_counts(sort=False)
ax = sns.lineplot(x=articles_by_month.index, y=articles_by_month, color='tab:orange')
ax.set(xlabel='Month', ylabel='Number of articles', xticks=np.arange(1,13))
plt.tight_layout()
plt.show()
plt.clf()
sentiment_camp = pd.crosstab(news_df.Political_camp, news_df.Sentiment, margins=True)
sentiment_camp.columns = ["Negative", "Neutral", "Positive", "All"]
quit
knitr::kable(py$sentiment_camp, digits = 4, caption="Polarities of the news articles on asylum seekers in Hong Kong in 2019")
reticulate::repl_python()
ax = sns.countplot(x="Racial_label", hue="Sentiment", data=news_df)
ax.set(xlabel="Presence of racial labels", ylabel="Number of articles")
ax.set_xticklabels(["No", "Yes"])
plt.show()
plt.clf()
fig, axes = plt.subplots(1, 2)
# Plotting distribution of title word count
sns.histplot(x='Title_length', data=news_df, ax=axes[0], color='tab:blue', alpha=0.5)
axes[0].set(xlabel='Word count', title='Article title')
mean_title_length = news_df.Title_length.mean()
axes[0].axvline(mean_title_length, alpha=0.5, linestyle = '-.', c='black', label='Mean of title length')
axes[0].legend()
# Plotting distribution of article word count
sns.histplot(x='Raw_article_length', data=news_df, ax=axes[1], color='tab:orange', alpha=0.5)
axes[1].set(xlabel='Word count (in thousands)', title='Raw article text')
mean_article_length = news_df.Raw_article_length.mean()
axes[1].axvline(mean_article_length, alpha=0.5, linestyle = '--', c='black', label='Mean of main text length')
axes[1].legend()
# Global setup
plt.tight_layout()
plt.show()
plt.clf()
article_length_summary = news_df[['Title_length', 'Raw_article_length']].describe()
quit
knitr::kable(py$article_length_summary, col.names = c("Title", "Raw main text"), caption="Summary statistics of the word counts of the news articles' titles and main texts")
reticulate::repl_python()
news_df["Political_camp"] = pd.Categorical(news_df["Political_camp"], categories=['Pro-Beijing', 'Neutral', 'Pro-democracy'])  # So that pro-Beijing becomes the reference category
news_df = pd.get_dummies(news_df, columns=["Political_camp", "Month"], drop_first=True)
news_df["Article"] = news_df.Title.str.cat(news_df.Text, sep=" ")
news_df.drop(columns=["Title", "Text"], inplace=True)
def read_text(path):
with open(path, 'r', encoding='utf-8') as file:
text = file.readlines()
text = [word.replace('\n', '') for word in text]
return text
hk_politics_words = read_text('Coding/HKPolDict-master/merged.txt') # Hong Kong politics
asylum_seeker_words = read_text('Coding/Asylum_seeker_words.txt') # Asylum seekers
for word in chain(hk_politics_words, asylum_seeker_words):
jieba.add_word(word)
# Creating the stop word list
stop_words_cantonese = read_text('Coding/text_cleaning/cantonese_stopwords.txt')
punctuations = [punc for punc in zhon.hanzi.punctuation]
stop_words_full = list(set(word for word in chain(stop_words_cantonese, punctuations)))
# tokenizer
def tokenize_zh(doc):
return jieba.cut(doc)
# Preprocessor
def preprocessor_zh(doc):
regex_punctuation = r"[\d+\s+\n\t]|[\s+\.\!\/_,$%^*(+\"\']+|[+——！，。？、~@#￥%……&*（）]+|[【】╮╯▽╰╭★→「」]+|[！，❤。～《》：（）【】「」？”“；：、]"
return re.sub(regex_punctuation, "", doc)
# Extracting the news article texts
news_articles = news_df.Article
from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer
from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer
countvec = CountVectorizer(preprocessor=preprocessor_zh, tokenizer = tokenize_zh, stop_words = stop_words_full, min_df = 0.02)
news_count_matrix = countvec.fit_transform(news_articles)
View(news_count_matrix)
from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer
countvec = CountVectorizer(preprocessor=preprocessor_zh, tokenizer = tokenize_zh, stop_words = stop_words_full, min_df = 0.02, binary=True)
news_count_matrix = countvec.fit_transform(news_articles)
View(news_count_matrix)
news_count_matrix.arctanh
