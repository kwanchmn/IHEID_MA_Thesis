return text
hk_politics_words = read_text('Coding/HKPolDict-master/merged.txt') # Hong Kong politics
asylum_seeker_words = read_text('Coding/Asylum_seeker_words.txt') # Asylum seekers
for word in chain(hk_politics_words, asylum_seeker_words):
jieba.add_word(word)
# Creating the stop word list
stop_words_cantonese = read_text('Coding/text_cleaning/cantonese_stopwords.txt')
punctuations = [punc for punc in zhon.hanzi.punctuation]
stop_words_full = list(set(word for word in chain(stop_words_cantonese, punctuations)))
# Modules
from sklearn.pipeline import Pipeline
from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer
from sklearn.decomposition import LatentDirichletAllocation, NMF
# tokenizer
def tokenize_zh(doc):
return jieba.cut(doc)
# Preprocessor
def preprocessor_zh(doc):
regex_punctuation = r"[\d+\s+\n\t]|[\s+\.\!\/_,$%^*(+\"\']+|[+——！，。？、~@#￥%……&*（）]+|[【】╮╯▽╰╭★→「」]+|[！，❤。～《》：（）【】「」？”“；：、]"
return re.sub(regex_punctuation, "", doc)
news_text = all_news_articles.Text
lda_countvec = CountVectorizer(preprocessor=preprocessor_zh,
tokenizer=tokenize_zh,
stop_words=stop_words_full,
min_df=0.02)
lda_term_matrix = lda_countvec.fit_transform(news_text)
lda_perplexity = {}
for i in range(1, 21):
lda_model = LatentDirichletAllocation(n_components=i, random_state=1, max_iter=100)
lda_model.fit(lda_term_matrix)
lda_perplexity[f"num of topics: {i}"] = lda_model.perplexity(lda_term_matrix)
lda_perplexity
news_text = all_news_articles.Text
lda_countvec = CountVectorizer(preprocessor=preprocessor_zh,
tokenizer=tokenize_zh,
stop_words=stop_words_full,
min_df=0.02)
lda_term_matrix = lda_countvec.fit_transform(news_text)
lda_perplexity = {}
for i in range(1, 21):
lda_model = LatentDirichletAllocation(n_components=i, random_state=1, max_iter=100, n_jobs=4)
lda_model.fit(lda_term_matrix)
lda_perplexity[f"num of topics: {i}"] = lda_model.perplexity(lda_term_matrix)
lda_perplexity
news_text = all_news_articles.Text
lda_countvec = CountVectorizer(preprocessor=preprocessor_zh,
tokenizer=tokenize_zh,
stop_words=stop_words_full,
min_df=0.02)
lda_term_matrix = lda_countvec.fit_transform(news_text)
lda_perplexity = {}
for i in range(1, 21):
lda_model = LatentDirichletAllocation(n_components=i, random_state=1, max_iter=100)
lda_model.fit(lda_term_matrix)
lda_perplexity[f"num of topics: {i}"] = lda_model.perplexity(lda_term_matrix)
lda_perplexity
nmf_tfidf = TfidfVectorizer(preprocessor=preprocessor_zh,
tokenizer=tokenize_zh,
stop_words=stop_words_full,
min_df=0.02)
nmf_term_matrix = nmf_tfidf.fit_transform(news_text)
nmf_beta_divergence = {}
for i in range(1, 21):
nmf_model = NMF(n_components=i)
nmf_model.fit(nmf_term_matrix)
nmf_beta_divergence[f"Number of topics: {i}"] = nmf_model.reconstruction_err_
print(nmf_beta_divergence)
import warnings
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import jieba
import zhon.hanzi
import re
from itertools import chain
pd.set_option('display.max_columns', None)
sns.set_theme(context='paper')
warnings.filterwarnings(action = 'ignore')
import warnings
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import jieba
import zhon.hanzi
import re
from itertools import chain
pd.set_option('display.max_columns', None)
sns.set_theme(context='paper')
warnings.filterwarnings('FutureWarning', action = 'ignore')
import warnings
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import jieba
import zhon.hanzi
import re
from itertools import chain
pd.set_option('display.max_columns', None)
sns.set_theme(context='paper')
warnings.filterwarnings(category='FutureWarning', action = 'ignore')
import warnings
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import jieba
import zhon.hanzi
import re
from itertools import chain
pd.set_option('display.max_columns', None)
sns.set_theme(context='paper')
warnings.filterwarnings(category=FutureWarning, action = 'ignore')
all_news_articles = pd.read_csv('Coding/all_news_articles.csv')
num_of_articles = all_news_articles.shape[0]
num_of_newspapers = all_news_articles.Newspaper.nunique()
quit
library(reticulate)
reticulate::repl_python()
import warnings
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import jieba
import zhon.hanzi
import re
from itertools import chain
pd.set_option('display.max_columns', None)
sns.set_theme(context='paper')
warnings.filterwarnings(category=FutureWarning, action = 'ignore')
all_news_articles = pd.read_csv('Coding/all_news_articles.csv')
num_of_articles = all_news_articles.shape[0]
num_of_newspapers = all_news_articles.Newspaper.nunique()
fig, axes = plt.subplots(1,2)
# Plotting by-outlet amount of articles
articles_outlet = all_news_articles.value_counts('Newspaper')
sns.barplot(x=articles_outlet, y=articles_outlet.index, ax=axes[0])
axes[0].set(xlabel='Number of articles', ylabel='', title='By news outlet')
for idx, value in enumerate(articles_outlet):
axes[0].text(value + 5, idx + 0.2, value)
# Plotting by-camp amount of articles
sns.countplot(x='Political_camp', data=all_news_articles, ax=axes[1])
axes[1].set(xlabel='', ylabel='Number of articles', title='By political camp')
count_by_political_camp = all_news_articles['Political_camp'].value_counts(sort=False)
for idx, value in enumerate(count_by_political_camp):
axes[1].text(idx - 0.1, value + 5, value)
axes[1].set_xticklabels(['Pro-Beijing', 'Neutral', 'Pro-democracy'], rotation=60)
plt.tight_layout()
plt.show()
plt.clf()
fig, axes = plt.subplots(1,2)
# Plotting by-month articles
articles_by_month = all_news_articles.Month.value_counts(sort=False)
sns.lineplot(x=articles_by_month.index, y=articles_by_month, color='tab:orange', ax=axes[0])
axes[0].set(xlabel='Month', ylabel='Number of articles', title='By month', xticks=np.arange(1,13))
# Plotting amount of articles before and during the anti-extradition law protest
sns.countplot(x='Protest', data=all_news_articles, ax=axes[1])
axes[1].set(xlabel='', ylabel='Number of articles', title='By before and during protest')
axes[1].set_xticklabels(['Before (Jan-May)', 'During (June-Dec)'])
plt.tight_layout()
plt.show()
plt.clf()
fig, axes = plt.subplots(1, 2)
# Plotting distribution of title word count
sns.histplot(x='Title_length', data=all_news_articles, ax=axes[0], color='tab:blue', alpha=0.5)
axes[0].set(xlabel='Word count', title='Article title')
mean_title_length = all_news_articles.Title_length.mean()
axes[0].axvline(mean_title_length, alpha=0.5, linestyle = '-.', c='black', label='Mean of title length')
axes[0].legend()
# Plotting distribution of article word count
sns.histplot(x='Raw_article_length', data=all_news_articles, ax=axes[1], color='tab:orange', alpha=0.5)
axes[1].set(xlabel='Word count (in thousands)', title='Raw article text', xticks=np.arange(0, 17e3, 2e3), xticklabels=np.arange(0, 17, 2))
mean_article_length = all_news_articles.Raw_article_length.mean()
axes[1].axvline(mean_article_length, alpha=0.5, linestyle = '--', c='black', label='Mean of main text length')
axes[1].legend()
# Global setup
plt.tight_layout()
plt.show()
plt.clf()
article_length_summary = all_news_articles[['Title_length', 'Raw_article_length']].describe()
quit
knitr::kable(py$article_length_summary, col.names = c("Title", "Raw main text"), caption="Summary statistics of the word counts of the news articles' titles and main texts")
reticulate::repl_python()
def read_text(path, output=None):
with open(path, 'r', encoding='utf-8') as file:
text = file.readlines()
text = [word.replace('\n', '') for word in text]
return text
hk_politics_words = read_text('Coding/HKPolDict-master/merged.txt') # Hong Kong politics
asylum_seeker_words = read_text('Coding/Asylum_seeker_words.txt') # Asylum seekers
for word in chain(hk_politics_words, asylum_seeker_words):
jieba.add_word(word)
# Creating the stop word list
stop_words_cantonese = read_text('Coding/text_cleaning/cantonese_stopwords.txt')
punctuations = [punc for punc in zhon.hanzi.punctuation]
stop_words_full = list(set(word for word in chain(stop_words_cantonese, punctuations)))
# Modules
from sklearn.pipeline import Pipeline
from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer
from sklearn.decomposition import LatentDirichletAllocation, NMF
# tokenizer
def tokenize_zh(doc):
return jieba.cut(doc)
# Preprocessor
def preprocessor_zh(doc):
regex_punctuation = r"[\d+\s+\n\t]|[\s+\.\!\/_,$%^*(+\"\']+|[+——！，。？、~@#￥%……&*（）]+|[【】╮╯▽╰╭★→「」]+|[！，❤。～《》：（）【】「」？”“；：、]"
return re.sub(regex_punctuation, "", doc)
# Extracting the news article texts
news_text = all_news_articles.Text
nmf_tfidf = TfidfVectorizer(preprocessor=preprocessor_zh,
tokenizer=tokenize_zh,
stop_words=stop_words_full,
min_df=0.02)
nmf_term_matrix = nmf_tfidf.fit_transform(news_text)
nmf_beta_divergence = {}
for i in range(1, 21):
nmf_model = NMF(n_components=i)
_ = nmf_model.fit(nmf_term_matrix)
nmf_beta_divergence[f"Number of topics: {i}"] = nmf_model.reconstruction_err_
print(nmf_beta_divergence, end='\n')
nmf_tfidf = TfidfVectorizer(preprocessor=preprocessor_zh,
tokenizer=tokenize_zh,
stop_words=stop_words_full,
min_df=0.02)
nmf_term_matrix = nmf_tfidf.fit_transform(news_text)
nmf_beta_divergence = {}
for i in range(1, 21):
nmf_model = NMF(n_components=i)
_ = nmf_model.fit(nmf_term_matrix)
nmf_beta_divergence[f"Number of topics: {i}"] = nmf_model.reconstruction_err_
print(nmf_beta_divergence, sep='\n')
print(nmf_beta_divergence)
print(nmf_beta_divergence, sep='\t')
print('09','12','2016', sep='-')
sns.lineplot(x=nmf_beta_divergence.keys(), y=nmf_beta_divergence.values())
plt.show()
plt.clf()
nmf_tfidf = TfidfVectorizer(preprocessor=preprocessor_zh,
tokenizer=tokenize_zh,
stop_words=stop_words_full,
min_df=0.02)
nmf_term_matrix = nmf_tfidf.fit_transform(news_text)
nmf_beta_divergence = {}
for i in range(1, 21):
nmf_model = NMF(n_components=i)
_ = nmf_model.fit(nmf_term_matrix)
nmf_beta_divergence[f"Topics(s): {i}"] = nmf_model.reconstruction_err_
sns.lineplot(x=nmf_beta_divergence.keys(), y=nmf_beta_divergence.values(), markers=True)
plt.show()
plt.clf()
sns.lineplot(x=nmf_beta_divergence.keys(), y=nmf_beta_divergence.values(), markers=True)
plt.gca().set_xticklabels(np.arange(1,21))
plt.show()
plt.clf()
sns.lineplot(x=nmf_beta_divergence.keys(), y=nmf_beta_divergence.values())
sns.scatterplot(x=nmf_beta_divergence.keys(), y=nmf_beta_divergence.values())
plt.gca().set_xticklabels(np.arange(1,21))
plt.show()
plt.clf()
sns.lineplot(x=lda_perplexity.keys(), y=lda_perplexitye.values())
sns.scatterplot(x=lda_perplexity.keys(), y=lda_perplexity.values())
plt.gca().set_xticklabels(np.arange(1,21))
plt.show()
plt.clf()
sns.lineplot(x=nmf_beta_divergence.keys(), y=nmf_beta_divergence.values())
sns.scatterplot(x=nmf_beta_divergence.keys(), y=nmf_beta_divergence.values())
plt.gca().set_xticklabels(np.arange(1,21))
plt.show()
plt.clf()
quit
library(reticulate)
reticulate::repl_python()
import warnings
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import jieba
import zhon.hanzi
import re
from itertools import chain
pd.set_option('display.max_columns', None)
sns.set_theme(context='paper')
warnings.filterwarnings(category=FutureWarning, action = 'ignore')
all_news_articles = pd.read_csv('Coding/all_news_articles.csv')
num_of_articles = all_news_articles.shape[0]
num_of_newspapers = all_news_articles.Newspaper.nunique()
fig, axes = plt.subplots(1,2)
# Plotting by-outlet amount of articles
articles_outlet = all_news_articles.value_counts('Newspaper')
sns.barplot(x=articles_outlet, y=articles_outlet.index, ax=axes[0])
axes[0].set(xlabel='Number of articles', ylabel='', title='By news outlet')
for idx, value in enumerate(articles_outlet):
axes[0].text(value + 5, idx + 0.2, value)
# Plotting by-camp amount of articles
sns.countplot(x='Political_camp', data=all_news_articles, ax=axes[1])
axes[1].set(xlabel='', ylabel='Number of articles', title='By political camp')
count_by_political_camp = all_news_articles['Political_camp'].value_counts(sort=False)
for idx, value in enumerate(count_by_political_camp):
axes[1].text(idx - 0.1, value + 5, value)
axes[1].set_xticklabels(['Pro-Beijing', 'Neutral', 'Pro-democracy'], rotation=60)
plt.tight_layout()
plt.show()
plt.clf()
fig, axes = plt.subplots(1,2)
# Plotting by-month articles
articles_by_month = all_news_articles.Month.value_counts(sort=False)
sns.lineplot(x=articles_by_month.index, y=articles_by_month, color='tab:orange', ax=axes[0])
axes[0].set(xlabel='Month', ylabel='Number of articles', title='By month', xticks=np.arange(1,13))
# Plotting amount of articles before and during the anti-extradition law protest
sns.countplot(x='Protest', data=all_news_articles, ax=axes[1])
axes[1].set(xlabel='', ylabel='Number of articles', title='By before and during protest')
axes[1].set_xticklabels(['Before (Jan-May)', 'During (June-Dec)'])
plt.tight_layout()
plt.show()
plt.clf()
fig, axes = plt.subplots(1, 2)
# Plotting distribution of title word count
sns.histplot(x='Title_length', data=all_news_articles, ax=axes[0], color='tab:blue', alpha=0.5)
axes[0].set(xlabel='Word count', title='Article title')
mean_title_length = all_news_articles.Title_length.mean()
axes[0].axvline(mean_title_length, alpha=0.5, linestyle = '-.', c='black', label='Mean of title length')
axes[0].legend()
# Plotting distribution of article word count
sns.histplot(x='Raw_article_length', data=all_news_articles, ax=axes[1], color='tab:orange', alpha=0.5)
axes[1].set(xlabel='Word count (in thousands)', title='Raw article text', xticks=np.arange(0, 17e3, 2e3), xticklabels=np.arange(0, 17, 2))
mean_article_length = all_news_articles.Raw_article_length.mean()
axes[1].axvline(mean_article_length, alpha=0.5, linestyle = '--', c='black', label='Mean of main text length')
axes[1].legend()
# Global setup
plt.tight_layout()
plt.show()
plt.clf()
article_length_summary = all_news_articles[['Title_length', 'Raw_article_length']].describe()
quit
knitr::kable(py$article_length_summary, col.names = c("Title", "Raw main text"), caption="Summary statistics of the word counts of the news articles' titles and main texts")
reticulate::repl_python()
def read_text(path, output=None):
with open(path, 'r', encoding='utf-8') as file:
text = file.readlines()
text = [word.replace('\n', '') for word in text]
return text
hk_politics_words = read_text('Coding/HKPolDict-master/merged.txt') # Hong Kong politics
asylum_seeker_words = read_text('Coding/Asylum_seeker_words.txt') # Asylum seekers
for word in chain(hk_politics_words, asylum_seeker_words):
jieba.add_word(word)
# Creating the stop word list
stop_words_cantonese = read_text('Coding/text_cleaning/cantonese_stopwords.txt')
punctuations = [punc for punc in zhon.hanzi.punctuation]
stop_words_full = list(set(word for word in chain(stop_words_cantonese, punctuations)))
# Modules
from sklearn.pipeline import Pipeline
from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer
from sklearn.decomposition import LatentDirichletAllocation, NMF
# tokenizer
def tokenize_zh(doc):
return jieba.cut(doc)
# Preprocessor
def preprocessor_zh(doc):
regex_punctuation = r"[\d+\s+\n\t]|[\s+\.\!\/_,$%^*(+\"\']+|[+——！，。？、~@#￥%……&*（）]+|[【】╮╯▽╰╭★→「」]+|[！，❤。～《》：（）【】「」？”“；：、]"
return re.sub(regex_punctuation, "", doc)
# Extracting the news article texts
news_text = all_news_articles.Text
lda_countvec = CountVectorizer(preprocessor=preprocessor_zh,
tokenizer=tokenize_zh,
stop_words=stop_words_full,
min_df=0.02)
lda_term_matrix = lda_countvec.fit_transform(news_text)
lda_perplexity = {}
for i in range(1, 21):
lda_model = LatentDirichletAllocation(n_components=i, random_state=1, max_iter=100)
_ = lda_model.fit(lda_term_matrix)
lda_perplexity[f"Topic(s): {i}"] = lda_model.perplexity(lda_term_matrix)
sns.lineplot(x=lda_perplexity.keys(), y=lda_perplexitye.values())
sns.scatterplot(x=lda_perplexity.keys(), y=lda_perplexity.values())
plt.gca().set_xticklabels(np.arange(1,21))
plt.show()
plt.clf()
lda_countvec = CountVectorizer(preprocessor=preprocessor_zh,
tokenizer=tokenize_zh,
stop_words=stop_words_full,
min_df=0.02)
lda_term_matrix = lda_countvec.fit_transform(news_text)
lda_perplexity = {}
for i in range(1, 21):
lda_model = LatentDirichletAllocation(n_components=i, random_state=1, max_iter=100)
_ = lda_model.fit(lda_term_matrix)
lda_perplexity[f"Topic(s): {i}"] = lda_model.perplexity(lda_term_matrix)
sns.lineplot(x=lda_perplexity.keys(), y=lda_perplexitye.values())
sns.scatterplot(x=lda_perplexity.keys(), y=lda_perplexity.values())
plt.gca().set_xticklabels(np.arange(1,21))
plt.show()
plt.clf()
lda_countvec = CountVectorizer(preprocessor=preprocessor_zh,
tokenizer=tokenize_zh,
stop_words=stop_words_full,
min_df=0.02)
lda_term_matrix = lda_countvec.fit_transform(news_text)
lda_perplexity = {}
for i in range(1, 21):
lda_model = LatentDirichletAllocation(n_components=i, random_state=1, max_iter=100)
_ = lda_model.fit(lda_term_matrix)
lda_perplexity[f"Topic(s): {i}"] = lda_model.perplexity(lda_term_matrix)
quit
library(reticulate)
exit
reticulate::repl_python()
import warnings
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import jieba
import zhon.hanzi
import re
from itertools import chain
pd.set_option('display.max_columns', None)
sns.set_theme(context='paper')
warnings.filterwarnings(category=FutureWarning, action = 'ignore')
all_news_articles = pd.read_csv('Coding/all_news_articles.csv')
num_of_articles = all_news_articles.shape[0]
num_of_newspapers = all_news_articles.Newspaper.nunique()
fig, axes = plt.subplots(1,2)
# Plotting by-outlet amount of articles
articles_outlet = all_news_articles.value_counts('Newspaper')
sns.barplot(x=articles_outlet, y=articles_outlet.index, ax=axes[0])
axes[0].set(xlabel='Number of articles', ylabel='', title='By news outlet')
for idx, value in enumerate(articles_outlet):
axes[0].text(value + 5, idx + 0.2, value)
# Plotting by-camp amount of articles
sns.countplot(x='Political_camp', data=all_news_articles, ax=axes[1])
axes[1].set(xlabel='', ylabel='Number of articles', title='By political camp')
count_by_political_camp = all_news_articles['Political_camp'].value_counts(sort=False)
for idx, value in enumerate(count_by_political_camp):
axes[1].text(idx - 0.1, value + 5, value)
axes[1].set_xticklabels(['Pro-Beijing', 'Neutral', 'Pro-democracy'], rotation=60)
plt.tight_layout()
plt.show()
plt.clf()
fig, axes = plt.subplots(1,2)
# Plotting by-month articles
articles_by_month = all_news_articles.Month.value_counts(sort=False)
sns.lineplot(x=articles_by_month.index, y=articles_by_month, color='tab:orange', ax=axes[0])
axes[0].set(xlabel='Month', ylabel='Number of articles', title='By month', xticks=np.arange(1,13))
# Plotting amount of articles before and during the anti-extradition law protest
sns.countplot(x='Protest', data=all_news_articles, ax=axes[1])
axes[1].set(xlabel='', ylabel='Number of articles', title='By before and during protest')
axes[1].set_xticklabels(['Before (Jan-May)', 'During (June-Dec)'])
plt.tight_layout()
plt.show()
plt.clf()
fig, axes = plt.subplots(1, 2)
# Plotting distribution of title word count
sns.histplot(x='Title_length', data=all_news_articles, ax=axes[0], color='tab:blue', alpha=0.5)
axes[0].set(xlabel='Word count', title='Article title')
mean_title_length = all_news_articles.Title_length.mean()
axes[0].axvline(mean_title_length, alpha=0.5, linestyle = '-.', c='black', label='Mean of title length')
axes[0].legend()
# Plotting distribution of article word count
sns.histplot(x='Raw_article_length', data=all_news_articles, ax=axes[1], color='tab:orange', alpha=0.5)
axes[1].set(xlabel='Word count (in thousands)', title='Raw article text', xticks=np.arange(0, 17e3, 2e3), xticklabels=np.arange(0, 17, 2))
mean_article_length = all_news_articles.Raw_article_length.mean()
axes[1].axvline(mean_article_length, alpha=0.5, linestyle = '--', c='black', label='Mean of main text length')
axes[1].legend()
# Global setup
plt.tight_layout()
plt.show()
plt.clf()
article_length_summary = all_news_articles[['Title_length', 'Raw_article_length']].describe()
quit
knitr::kable(py$article_length_summary, col.names = c("Title", "Raw main text"), caption="Summary statistics of the word counts of the news articles' titles and main texts")
reticulate::repl_python()
def read_text(path, output=None):
with open(path, 'r', encoding='utf-8') as file:
text = file.readlines()
text = [word.replace('\n', '') for word in text]
return text
hk_politics_words = read_text('Coding/HKPolDict-master/merged.txt') # Hong Kong politics
asylum_seeker_words = read_text('Coding/Asylum_seeker_words.txt') # Asylum seekers
for word in chain(hk_politics_words, asylum_seeker_words):
jieba.add_word(word)
# Creating the stop word list
stop_words_cantonese = read_text('Coding/text_cleaning/cantonese_stopwords.txt')
punctuations = [punc for punc in zhon.hanzi.punctuation]
stop_words_full = list(set(word for word in chain(stop_words_cantonese, punctuations)))
# Modules
from sklearn.pipeline import Pipeline
from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer
from sklearn.decomposition import LatentDirichletAllocation, NMF
# tokenizer
def tokenize_zh(doc):
return jieba.cut(doc)
# Preprocessor
def preprocessor_zh(doc):
regex_punctuation = r"[\d+\s+\n\t]|[\s+\.\!\/_,$%^*(+\"\']+|[+——！，。？、~@#￥%……&*（）]+|[【】╮╯▽╰╭★→「」]+|[！，❤。～《》：（）【】「」？”“；：、]"
return re.sub(regex_punctuation, "", doc)
# Extracting the news article texts
news_text = all_news_articles.Text
lda_countvec = CountVectorizer(preprocessor=preprocessor_zh,
tokenizer=tokenize_zh,
stop_words=stop_words_full,
min_df=0.02)
lda_term_matrix = lda_countvec.fit_transform(news_text)
lda_perplexity = {}
for i in range(1, 21):
lda_model = LatentDirichletAllocation(n_components=i, random_state=1, max_iter=100)
_ = lda_model.fit(lda_term_matrix)
lda_perplexity[f"Topic(s): {i}"] = lda_model.perplexity(lda_term_matrix)
quit
